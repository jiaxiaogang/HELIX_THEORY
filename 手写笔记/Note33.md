# (觅食 & 飞躲 & 踢搬运)三项的多向连续训练,以及三项融合训练

在前面,已经学会搬运,且会用搬运来踢坚果到路上,至此所有单轮的训练项目全部通过了,以下开始对这些项目进行连续行为训练,以及融合在一起训练等;

***

<!-- TOC -->

- [(觅食 & 飞躲 & 踢搬运)三项的多向连续训练,以及三项融合训练](#觅食--飞躲--踢搬运三项的多向连续训练以及三项融合训练)
  - [n33p01 回顾多向觅食训练之: (FZ981学无距有向果场景B & FZ982各方向学飞吃)](#n33p01-回顾多向觅食训练之-fz981学无距有向果场景b--fz982各方向学飞吃)
  - [n33p01b 回顾多向觅食训练之: FZ983试错训练](#n33p01b-回顾多向觅食训练之-fz983试错训练)
  - [n33p02 完善TCPlanV2流程细节](#n33p02-完善tcplanv2流程细节)
  - [n33p03 回顾多向觅食训练之: 继续FZ983试错训练](#n33p03-回顾多向觅食训练之-继续fz983试错训练)

<!-- /TOC -->

***

## n33p01 回顾多向觅食训练之: (FZ981学无距有向果场景B & FZ982各方向学飞吃)
`CreateTime 2024.08.09`

训练就两个方法: 一是教的多,二是练的多;

| 33011 | 尝试训练觅食-初步规划大岗 |
| --- | --- |
| 1. 学飞 | 8个方向一步远位置各扔下无皮果,引至吃掉 |
| 2. 试错 | 8个方向一步远位置各扔下无皮果,点击饥饿,试错尝试; |

| 33012 | 尝试训练觅食-训练方法分析 |
| --- | --- |
| 原则 | **训练就俩方法: 一是教,二是练**; |
| 教 | 即: 最基本的经历(比如: 能够抽象出无距方向坚果的场景 & 在反射反应中各个方向都飞过); |
| 练 | 即: 先给个简单的的场景(比如: 坚果扔近些,让它试错,让正确飞行方向的Canset能优胜出); |

| 33013 | 尝试训练觅食-学无距有向果场景 |
| --- | --- |
| 目标 | 先习得`8个方向无距坚果`的场景; |
| 回顾 | 经查871,882,913,这前三步的训练,都没有各方向无皮果训练的步骤; |
| 步骤 | **无皮学饿: 认知模式(`饿(持续饿感3次),扔随机无皮坚果,不吃,又饿`x50轮);** => |
| 强训 | 参考RLT工具中现在已经有`学饿`和`学吃`两个强训项 (参考28172-第1步&第2步); |

**小结: 上面三表,从大岗到实际尝试`训练觅食`,测得时序识别同质化严重的问题,转下表先修一下;**

| 33014 | BUG: 测得时序识别结果同质化严重 |
| --- | --- |
| 描述 | 在上表训练中,时序识别结果20条全是`[饿]->{更饿}`; |
| 问题 | 而我们希望它能训练出`无距有向果`场景,现在连识别都识别不到,谈何抽象出`无距有向果`场景呢? |
| 白话 | 说白了,这会导致智能体刻板固化,不易接受新事物(新场景难出头,更难类比抽象); |
| 说明 | 它识别的全是[饿]->{更饿},并且它们强度一个比一个强,所以`带方向果`的永无出头之日; |
| 分析 | 现在时序没有防重功能,导致同质化严重,同样的饿->更饿能占满20条,即因为此; |
| 方案1 | 场景时序加上全局防重功能 `95%`; |
|  | > 全局防重倒是简单可行,只需要参考当时概念全局防重: 在构建时,优先取本地已有的一模一样的时序即可; |
| 方案2 | 同质时序在识别结果中互斥 `5%`; |
|  | > 互斥很难判断,比如indexDic全含,且有共同的抽具象关系,算互斥吗?或者说相似度达到怎样算互斥? |
| 实践 | 暂选定方案1进行实践: 参考概念全局防重,为场景时序构建时,也加上相应的防重功能即可; |
| 结果 | 经查,原本就有防重功能,只是构建`最具象时序protoFo`时,有些没调用防重方法,全改成调用即可 `T`; |

**小结: 33014中,把时序识别同质化严重的问题修了下 (打开了protoFo构建时的全局防重),下表继续回归训练觅食;**

| 33015 | FZ981-尝试训练觅食-学无距有向果场景B |
| --- | --- |
| 说明 | 因为protoFo未防重的BUG影响到FZ97,所以本表重新从第1步训练FZ98; |
| FZ981 | 用强训工具训练`无皮学饿` (参考33013-步骤) `存为FZ981`; |
| 训练步骤 | `饿,扔随机坚果,不吃,又饿` x 50轮; |
| 回测 | `FZ981,饿,随便扔个果`,观察概念识别和时序识别结果,如下: |
|  | 1. 概念识别结果中: 无向果 或 无距果 两种都有了; |
|  | 2. 时序识别结果中: 无向果场景 或 无距果场景 或 无距无向果场景 三种都有了 (参考33015-代码段1); |
| 问题 | 虽然A和F识别都ok了,不过时序识别在二次过滤后: 未识别到`无距有向果场景`结果 (参考33015-代码段2); |
| 结果 | 将问题修复后,重新训练FZ981完成 `T`; |

```java
33015-代码段1: 训练日志分析 ==> 这个代码段主要是33015-回测的从时序识别日志中摘出;
经查1: 从时序识别结果看: protoFo:[饿果]-> 和 protoFo:[饿果果]-> 没有识别结果 (时序识别到0条);
经查2: 反而,饿果果果 & 饿果果果果 & 饿果果果果果 & 饿果果果果果果 & 饿果果果果果果果 & 饿果果果果果果果饿果 & 饿果果果果果果果饿果果 ... 都有识别结果;
    > 应该是因为只识别有mv指向的结果,而连续视觉中,只要更饿发生了,就肯定已经看了3帧以上的`果`了,前两帧时,还没更饿,所以也没法识别到,这是正常的;

成果1: 像无向果场景也可以识别到 => "2. P强度:(106)    > F1951[M1{↑饿-16},A1950(距90,果),A1950(距90,果),A1950(距90,果)]->{-6.54}";
成果2: 像无距果场景也可以识别到 => "0. P强度:(2060)    > F31[A30(向189,果),A30(向189,果),A30(向189,果),A30(向189,果)]->{-4.29}";
成果3: 像无向无距果场景也可以识别到 => "3. P强度:(289)    > F1654[M1{↑饿-16},A49(,果),A49(,果),A49(,果)]->{-2.49}";
结果: 如上日志3个成果可见,时序识别已经ok了,无距有向果场景,已经抽象到了,也能识别到了 `T`;
```

```java
33015-代码段2: 测得识别问题 ==> 在时序识别二次过滤后,没有`无距有向果场景`的结果;
时序二次过滤后条数: 原20 剩5 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
	1. F956[M1{↑饿-16},A932(距90,向161,果),A932(距90,向161,果),A932(距90,向161,果)]
	2. F935[M1{↑饿-16},A932(距90,向161,果),A932(距90,向161,果),A932(距90,向161,果)]
	3. F443[M1{↑饿-16},A436(距67,向174,果),A436(距67,向174,果),A436(距67,向174,果),A436(距67,向174,果),A436(距67,向174,果),A436(距67,向174,果),A436(距67,向174,果)]
	4. F52[M1{↑饿-16},A51(向182,果),A14(距121,向170,果),A14(距121,向170,果),A14(距121,向170,果),A14(距121,向170,果),A14(距121,向170,果),A14(距121,向170,果),A14(距121,向170,果)]
	5. F28[M1{↑饿-16},A12(距100,向182,果),A14(距121,向170,果),A14(距121,向170,果),A14(距121,向170,果),A14(距121,向170,果),A14(距121,向170,果),A14(距121,向170,果),A14(距121,向170,果)]
结果: 经查是因为二次过滤算法,只针对PJ似层,把交层全给过滤掉了 `已修复 T`;
```

**小结: 以上33015做了第1步FZ981学饿,并且已经可以顺利抽象出`无距有向果场景`,下面继续在这个基础上做第2步`学吃训练`**

| 33016 | FZ982-尝试训练觅食-各方向学飞吃 |
| --- | --- |
| FZ982 | 学一下向8个方向的飞,和靠飞可以改变距离,解决吃到坚果的距离问题,存为FZ982 |
| 步骤初定 | 手动训练: 8个方向一步远位置各扔下无皮果,引至吃掉 |
| 步骤分析 | 刚开始还是多做些原始积累,用强化训练来跑比较好,并且手把手教的太多也不太好,像婴儿一样让它自己学步比较好; |
| 训练步骤 | `饿,扔附近坚果,随机连飞3下(如飞中吸吮反射)` x 100轮 (参考28173-2学飞); |
| 重点关注 | 打出flt日志,观测在预测会更饿,然后飞吃到坚果后,习得newRCanset; |
| 结果 | 经查日志,newRCanset执行了很多,因为是认知模式,没有跑absRCanset,等FZ983时,再测absRCanset; |

**小结: 做了FZ982训练完成,下面做FZ983试错训练;`**

***

## n33p01b 回顾多向觅食训练之: FZ983试错训练
`CreateTime 2024.08.18`

| 33017 | FZ983-尝试试错训练-各方向自行飞吃 |
| --- | --- |
| FZ983 | 8个方向方向扔下无皮果,然后让它自己决策行为飞吃; |
| 训练步骤1 | 附近一飞距离手动扔无皮果,然后观察它能不能越来越准确的飞正确方向; |
| 训练步骤2 | 远距离手动扔无皮果,然后观察它能不能自行准确的飞吃到坚果; |
| 重点关注 | 1.观察下各个RCanset的SP值; 2.观察下在求解时RCanset正确时,能触发类比并生成AbsRCanset; |
| 训练记录 | 第1次: 饿,扔右上无皮果,它自行右下飞,上飞,上飞,吃了果; |
|  | 第2次: 饿,扔上方无皮果,它自行上飞,吃了果; |
| 明日 | 重点观察下,让sceneFrom和sceneTo都是带无距有向果的场景,不然正确再多,也只是蒙中了而已; |
|  | 或者,即使Root是无向的,它的子H任务也得有向 (必须有向场景,才能有正确的Canset行为化,才能输出正确飞的方向); |

**小结: 试错训练并不顺利,下面几个BUG都是因为试错训练不顺利来的;**

```java
33018-BUG回测发现RCanset跳过了飞 `T 不是BUG,见结果`
=============================== 5 rSolution ===============================
任务源:饿 protoFo:F5923[M1{↑饿-16},A3442(向135,距12,果),M1{↑饿-16},A3442(向135,距12,果)] 已有方案数:0 任务分:-3.93
第7步 R排除Infected传染掉的:53
R0. I<F1418 F3788[↑饿-16,2果,飞↙,2果]> {0 = 0;1 = 1;2 = 3;}  H4N0:(分:1.00) [CUT:3=>TAR:4]
R1. I<F1568 F3789[↑饿-16,3果,飞↙,3果]> {0 = 0;1 = 1;2 = 3;}  H4N0:(分:1.00) [CUT:3=>TAR:4]
复现: FZ982,饿,附近扔无皮果 -> 然后看rSolution日志即可看到;
说明: 如上日志,所有RCanset的CUT就是第四帧(下标3),而第四帧就是最后一帧,即它没行为化任何东西,直接等待末帧mv自然不发生;
问题: 这显然不对,这种RCanset没有飞行,是不可能成功的,它的前段条件判断有问题;
回顾: 上次原本就把前段条件满足给弃掉了,所以这个BUG我们正好想一下,是否应该重新打开前段条件满足 (上回是用场景满足替代了前段条件满足);
思路: 查下,为什么这里的CUT是3,它应该是1才对吧;
    > 日志1. sceneFrom: F1568[M1{↑饿-16},A1563(距45,果),A1563(距45,果)]
    > 日志2. cansetFrom: F3789[M1{↑饿-16},A1563(距45,果),飞↙,A1563(距45,果)]
说明: 如上日志可见: sceneFrom中确实发生两帧无皮果,所以对应到cansetFrom确实是在方向之后了,没啥BUG;
    > 日志3. sceneFrom: F1418[M1{↑饿-16},A51(,果),A51(,果)]
    > 日志4. cansetFrom: F1841[M1{↑饿-16},A51(,果),A51(,果),飞↘,A1838(距7,向286,果)]
说明: 如上日志可见: 也有些sceneFrom的cansetFrom中的飞是在CUT之后的 (即可以正常输出飞行行为的);
结果: 所以,此BUG不算BUG,只是有一些无效的canset还没被否掉罢了,等它们都被否了,自然就有可以输出飞行行为的canset可被激活 `T 不是BUG,不必修,继续33017试错训练`;
```

| 33019 | BUG2-Root无法持续激活,导致来不及p反省负SPEFF,就不激活了 |
| --- | --- |
| 起因 | 试错训练时,无向场景,下的cansetFrom应该被SP否掉,直至无法激活; |
|  | 但训练时: RCanset:F1418被激活许多次,也没能飞对方向解决饥饿问题,但P反省的负SPEFF日志一直没执行到; |
| 说明 | 现在在持续视觉后,其实像[无皮果,飞,吃]这样的Canset里的这几帧都很容易被反馈到,但它即使全反馈了也未必有效; |
|  | 比如: 一个无向场景,它下面的Canset可能飞向任何方向,但这个方向是错误的,此时即使全反馈到Canset执行完成,也无效; |
| 问题 | 这导致一个Root即使R帧推进再顺利,P末帧反馈也无效,但现在的BUG是`它没有执行到P反省负SPEFF`; |
| 调试 | Root1激活后,执行一两轮行为化,大概能飞一下,然后就成了被传染状态,在计算任务进度分时,传染状态会跳过; |
|  | 导致,Root1被传染了,它的进度分是0,也就没啥竞争力,导致无法激活了,此时还没到执行p反省的时候,所以没法计负; |
|  | 导致,试错训练中,无论怎么跑,它都`没起到试错的作用`; |
| 日志 | R行为化中间帧下标 (3/5) 飞↑ from时序:F2236[↑饿,果,果,飞↑,A2233(果)] fromDemand:F8414 |
|  | R行为化中间帧下标 (4/5) A2233(距0,向103,果) from时序:F2236[↑饿,果,果,飞↑,A2233(果)] fromDemand:F8414 |
|  | R行为化末帧下标 (4/4)  from时序:F6341[M1{↑饿-16},A51(,果),飞↑,A51(,果)] fromDemand:F8428 |
|  | 说明: Root任务F8414激活两次后,就没再激活了; |
| 思路 | 说白了,还是Root任务的竞争问题,以前的做法是:`任务1静默等待时,会执行任务2`; |
|  | 问题-而这一做法导致饥饿Root1静默了,饥饿Root2继续尝试,依此类推,没一个Root能跑到P反省负SPEFF; |
| 方案1 | 同区任务应该互斥,非同区任务不排斥; |
|  | 1. 情况1-即一个饥饿任务在静默等待时,别的饥饿任务也不应该执行; |
|  | 2. 情况2-而饥饿任务在静默等待时,非饥饿的任务应该可以先执行; |
| 方案2 | 所有任务都不排斥; |
|  | 1. 在此方案下,如果饥饿任务最迫切并竞争取胜激活了,并且此任务在静默等待状态,那么别的任务全等着不允许执行; |
|  | 2. 除非`危险任务`更强,它最迫切并取胜激活了,才可以跨过饥饿,先执行危险任务; |
| 抉择 | 方案1看似更应对所有情况,实际方案2更简单直白,方案2也可以应对所有情况,所以先选用方案2; |
|  | 如果以后方案2发现不够用,再回来考虑方案1; |
| 实践 | TODO1-在Canset被传染后,或者状态非ActYes或Runing时,也计进度分 `T`; |
|  | TODO2-在TCPlan中,如果Root1是静默状态(或者被传染状态),那么这轮TO线程就空转啥也不跑; |
| 结果 | TODO1做了,而TODO2的问题在于`TCPlan在各种状态下要跑哪个`,这个 `转n33p02里整体完善下细节 T`; |

* **3301a: BUG3-接上表,即使Root可以持续激活,它的RCanset也没能持续激活,而是激活的RCanset在不断变化;**
- 问题: 这种RCanset不断变化,会导致它不断左突右试,各种乱飞,即使这些RCanset全都不对;
- 目标: 查下这个BUG,让它能随着稳定的一个RCanset从执行到反省SPEFF,该静默就静默等待,等一条噶了,再试下一条;
- 思路: 调试下代码,查下为什么RCanset在不断变 (以下以行为化F2236为例,查它为什么在执行行为化后,又噶了);
  - `R行为化中间帧下标 (3/5) 飞↑ from时序:F2236[M1{↑饿-16},A51(,果),A51(,果),飞↑,A2233(距0,向103,果)]`
- 调试: 以下两种情况(F2236被改成了ActNo或WithOut状态),这两种状态都会导致在实时竞争中,它直接被过滤掉 (噶掉);
  - 调试情况1. 有一次是第四帧A2233果超时了,它自然在OR反省中,计为ActNo状态了;
    - `在ReasonOutRethink反省后 solution:F2236 因超时无效而set actYes to actNo————>`
    - 分析: 这种情况下,F2236应该计了负SPEFF,是正常的;
  - 调试情况2. 还有两次,是因为第三帧飞,没生成SubHDemand (导致了在TCPlan.112行中,它因无HDemand直接计为WithOut状态 ();
    - 分析: 这种情况应该不正常,像F2236的第三帧是飞行为输出,它是不会生成子HDemand任务的;
- 结果: 这里像`中间帧超时在TCPlanV2的处理,或者中间帧是Out行为在TCPlanV2中的处理`等等 `转n33p02完善下TCPlanV2的细节 T`;

**小结: 本节试错训练不顺利,在试错训练中,测到多个bug,最终指向了前段时间迭代的TCPlanV2还有一些细节问题,转下表继续;**

***

## n33p02 完善TCPlanV2流程细节
`CreateTime 2024.08.27`

在上节33019&3301a的结果中,测得多种情况或状态下,TCPlanV2现代码对这些情况处理的不太清晰,所以本节把TCPlanV2整理下各种行为化情况,时下一步走向哪里,完善完善细节 (因为原来迭代TCPlanV2时是单纯的迭代完了,却没测,现在算是测下,并看哪里有问题,完善下细节);

```java
33021-现在测到的要完善哪些细节问题;
回顾: 现TCPlanV2代码没考虑末帧时的情况,也没考虑中间帧是Out时的情况,本节都补上;
说明: 下面是把这些完善细节后的伪代码整理如下:
//6. 三种情况,分别走三块不同逻辑;
if (canset已经到了末帧) {
    //一. ================================ 末帧 ================================

    if (bestCanset.feedbackMvAndPlus)
    //11. 好的mv反馈,说明当前rRootDemand被解决了,不需要再决策 => 继续尝试下一root;

    else if (bestCanset.feedbackMvAndSub && 是持续性任务)
    //12. 坏的mv反馈: 如果是持续性任务,则该canset失败,继续尝试下一canset;

    else if (bestCanset.feedbackMvAndSub && 非持续性任务)
    //13. 坏的mv反馈: 如果非持续性任务,则该root失败 => 继续尝试下一root;

    else if (!bestCanset.actYesed && !bestCanset.feedbackMv)
    //14. 还在等待mv反馈中 => 则继续等待即可;

    else if (bestCanset.actYesed && !bestCanset.feedbackMv)
    //15. 等待结束,避免负mv成功,则该任务完成 => 继续尝试下一root;
} else if (frameAlg.content_p.isOut) {
    //二. ================================ 中间帧_Out ================================

    if (frameAlg.actYesed && !frameAlg.feedbackAlg) //21. actYesed && !feedbackAlg -> 当前行为输出到期也没等到反馈: 把当前bestCanset否掉,重新找出下一个bestCanset,转下一个canset;

    if (frameAlg.feedbackAlg) //22. feedbackAlg -> 则应该在feedbackTOR()中已经转了下一帧,但如果这里如果取curFrame,发现有反馈,还没转,则先不管它,啥不也不执行吧,等它自己转下一帧 (不管什么状态,只要已经反馈了,就都走这里);

    if (!frameAlg.actYesed) //23. 等待中的isOut帧,没有subH,只需要等肢体动作执行完成再转输入rInput后,会反馈成功,还在等待说明还没触发,继续等着即可 (行为输出也是需要时间的,比如飞要0.2s,再静默等等) (参考3301a-调试情况2);

} else {
    //三. ================================ 中间帧_非Out ================================

    if (frameAlg.actYesed && !frameAlg.feedbackAlg) //31. actYesed && !feedbackAlg -> 当前行为输出到期也没等到反馈: 把当前bestCanset否掉,重新找出下一个bestCanset,转下一个canset;

    if (frameAlg.feedbackAlg) //32. feedbackAlg -> 则应该在feedbackTOR()中已经转了下一帧,但如果这里如果取curFrame,发现有反馈,还没转,则先不管它,啥不也不执行吧,等它自己转下一帧 (不管什么状态,只要已经反馈了,就都走这里);

    if (!subHDemand) //33. 非Out帧等待中,则尝试subH求解 -> 防空检查 (非输出帧的subH不应该为空,没有hDemand是BUG,因为algModel初始时,就有hDemand了) (subH为空时,那这条Canset失败,继续尝试baseDemand的下一条 (逐条尝试))

    if (!subHDemand.alreadyInitCansetModels) //34. subH没求解过,则尝试对subH求解: 成功: 当前条 -> hDemand没初始化过,直接return转hSolution为它求解;

    //35. subH求解过,则subH继续深入下一层: 继续: 下一层 -> 当前条继续向枝叶规划;
    BOOL success = [self plan4Cansets:subHDemand complate:complate prefixNum:prefixNum + 2];

    if (!success && !frameAlg.actYesed) //36. 如果subH求解全失败了,则咱不解了,咱等着即可,看它能不能自行反馈到,则继续等 -> 如果bestCanset枝叶全失败了,还是静默等等状态,直接返回成功,啥也不用干 (比如: 等饭熟,有苹果也会先吃一个垫垫);

    if (!success && frameAlg.actYesed) //37. 如果subH求解全失败了,它的等待时间也结束了,则当前bestCanset计为失败: 驳回: 下一条 -> 当前hDemand的枝叶全失败了,继续尝试baseDemand的下一条 (逐条尝试);
}
```

**总结: 本节完善了TCPlanV2的细节,下表继续回归FZ983试错训练;**

***

## n33p03 回顾多向觅食训练之: 继续FZ983试错训练
`CreateTime 2024.08.30`

上节完善了TCPlanV2的细节,本节回归多向觅食训练;

| 33031 | 在FZ982的基础上,继续做试错训练,并观察日志还有没什么问题 |
| --- | --- |
| BUG1 | 测得SP有重复计数的问题 (比如饥饿任务求解中,又更饿了很多次,此时末帧S就重复执行了多次) `T`; |
|  | fix1: 写一个方法,防止SP计数重复 (只记一次) `T 参考checkAndUpdateOutSPStrong()`; |
| BUG2 | 测得SP有冲突计数的问题 (比如无皮果没反馈计了S,过了一会又反馈了计了P,此时S要回滚掉) `T`; |
|  | 回顾: 以前就有回滚,但写的比较乱,执行起来也许多问题 (比如重复执行多次,只回滚一次等问题); |
|  | fix2: 改为写个方法S/P换了时,就把已经记录的P/S回滚 `T 参考checkAndUpdateOutSPStrong()`; |
| 问题3 | rCanset在推进到中途时,提前反馈了负mv 或 正mv,此时是否应该提前为末帧SP计数; |
|  | 方案: 改为在正mv 或 负mv发生时,即使canset没推进到末帧,也把feedbackMv记录下来,并且为末帧SP计数+1; |
|  | TODO3: 把pInput中正mv输入给持续性任务后调用feedbackTOP 和 feedbackTOP中改为不再限制必须已经到末帧; |
|  | TODO4: scene预测也要支持mv提前反馈的情况 (即feedbackTIP支持); |
|  | 回顾: 现在的做法是: 当mv提前发生时,会为其生成新的scene或canset,并触发类比抽象 `T 所以此问题不必解决`; |
|  | 结果: 即提前反馈mv可以不处理,为其构建新fo,并且触发类比构建新的absFo,也是一样的,何必支持提前反馈多此一举 `T`; |
|  | 总结: 问题3已经分析了方案和TODOLIST,但在代码中又发现,这个不需要修 `T 不用修`; |
| 结果 | 本表中,修复了前两个问题,第3个问题不算问题 `T`; |

| 33032 | 试错训练步骤 |
| --- | --- |
| 说明 | 前面修了好几个试错SP计数等BUG,本表继续搞试错训练; |
| 步骤 | 饿,附近扔无皮果,让它自己试飞,如果飞错飞远了,就把它再给拉到附近来,让它继续试; |
| 注意 | 1. 观察一下scene识别结果中,有没有`有向无距场景`; |
|  | 2. 观察一下canset竞争因为SP计数变化,直接影响到竞争力变化; |
| BUG1 | 吃到一个坚果后,它还在飞,按道理说,正mv反馈会导致所有的饥饿Root任务全失效,应该不会再飞了; |

<br><br><br><br><br>
