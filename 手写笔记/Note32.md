# 训练和修细节

***

<!-- TOC -->

- [训练和修细节](#训练和修细节)
  - [n32p01 试错效率低下问题: 新增OutSPDic](#n32p01-试错效率低下问题-新增outspdic)
  - [n32p01b 回测](#n32p01b-回测)
  - [n32p02 多层子H任务嵌套时的H迁移](#n32p02-多层子h任务嵌套时的h迁移)
  - [n32p03 整理下：HE模型流程图](#n32p03-整理下he模型流程图)
  - [n32p04 调整"持续饿感"的"任务失效机制" : 调整为负mv反馈后任务不失效](#n32p04-调整持续饿感的任务失效机制--调整为负mv反馈后任务不失效)
  - [n32p05 学搬运](#n32p05-学搬运)
  - [n32p06 用搬运（一）](#n32p06-用搬运一)
  - [n32p07 迭代TCPlanV2](#n32p07-迭代tcplanv2)
  - [n32p08 用搬运（二）](#n32p08-用搬运二)
  - [n32p09 "搬运,去皮,飞至,吃掉": 连起来跑通](#n32p09-搬运去皮飞至吃掉-连起来跑通)
  - [n32pxx 连续视觉](#n32pxx-连续视觉)
  - [n32pxx TODO备忘](#n32pxx-todo备忘)

<!-- /TOC -->

***

## n32p01 试错效率低下问题: 新增OutSPDic
`CreateTime 2024.06.09`

```txt
32011-我特意加训了rCanset[饿,扔无皮果,飞至,吃],但rSolution就是激活不到它;
训练: 在FZ954x7基础上: 加训饿果飞吃三次后,存为:FZ955x3;
说明: 如下日志,rSolution激活到的,都是一些[硬扛],或者[饿,果,果],这些rCanset;
3. I<F4900 F8772[M1{↑饿-16},M1{↑饿-16},M1{↑饿-16},M1{↑饿-16}]> {0 = 3;} {} (null):(分:0.00)
4. I<F4900 F8798[M1{↑饿-16},M1{↑饿-16},M1{↑饿-16},M1{↑饿-16}]> {0 = 3;} {} (null):(分:0.00)
3. I<F3611 F6351[A13(饿16,7),A4899(距11,果),A4899(距11,果)]> {} {0 = S4P2;1 = S0P1;2 = S0P1;} H2N6:(分:0.25)
4. I<F3521 F5100[A13(饿16,7),A5099(向90,果),A5099(向90,果)]> {} {0 = S0P1;1 = S1P2;2 = S3P2;} H3N3:(分:0.50)
4. I<F3521 F5100[A13(饿16,7),A5099(向90,果),A5099(向90,果)]> {} {0 = S0P1;1 = S1P2;2 = S3P2;} H3N3:(分:0.50)
解决思路: 说白了,无论是错误还是正确的rCanset,都没有积累过SPEFF,导致对的没出头,错的又易胜出,所以搞下试错训练自然就好了,如下:
试错训练: `FZ954x7,饿,上方扔无皮果`,如果它激活硬扛等错误rCanset就会败北,如果它激活[无皮果,飞,吃]就能成功解;
具体实行: 打出flt日志,step1=激活行为化的rCanset,step2=feedbackTOR反馈,step2b=feedbackTOP反馈,step3a=OR反省,step3b=OP反省,从这5个日志观察整个rCanset从激活到最终SP反馈;
遇到问题: 在跑以上试错训练时,发现试错训练有点慢,虽然传染了,但只要SP不打负分,就相当于每次遇到任务时,都得重新来一遍 `此问题转32012`;
结果: 在32012写了OutSPDic后,Canset可以快速响应了,试错效率没啥问题了 `T 参考32012-结果`;
```

**小结: 上表在做试错训练,但并不顺利,问题如下 (此问题转下表解决,待解决后,再回来继续进行试错训练);**  

**问题: 因为SPEFF仅针对cs_besting的canset,导致那些cs_none状态的,在下次重启时又可以卷土重来,而真正有用的canset[无皮果,飞,吃]则很难有出头之日 (即试错效率太低);**

| 32012 | Canset池SPEFF试错效率太低 |
| --- | --- |
| 说明 | 见上表试错训练时-遇到问题: 传染只发生在工作记忆中,长时记忆的SPEFF又仅针对转实后的Canset,导致效率低下; |
| 方案 | 即使不转实,也可以累积SPEFF,这样可以从根本上解决试错效率低下的问题; |
| 实践 | 可以在sceneTo下,直接针对sceneFrom和cansetFrom记录SPDic,这样性能才ok,可不转实就批量记录SP参数; |
|  | > 借此机会把IN和OUT的SPDic分开 (IN是对scene存SP,OUT是对canset存SP); |
| TODO1 | 新写一个AIOutSPStrong(),把sceneFrom和cansetFrom存里面,然后把spDic也存里面 `T`; |
| TODO2 | 在AIFoNodeBase里加一个outSPDic<K=sceneFromPId_cansetFromPId, V=AIOutSPStrong> `T`; |
| TODO3 | 改下在生成xvModel前(更不需要等siModel),就把outSPDic初始一下 `T`; |
|  | 时机: 在构建canset到canset池时,把cansetFrom的spDic做为初始化outSPDic (加上防重,仅初始化一次) `T` |
|  | 更正: 把由spDic做初始,改成由sceneFrom中cansetFrom的outSPDic来初始 `T`; |
| TODO4 | 改下把中间帧超时反馈失败的 (及所有传染到的),在actYes超时未反馈后,计SP- `T`; |
| TODO5 | 改下把中间帧反馈成功的 (或被唤醒的),在feedbackTOR反馈匹配后,计SP+ `T`; |
|  | 另外: 其中唤醒的,应先把传染时的负1回滚一下,再把正1加上 `T`; |
| TODO6 | 改下把末帧超时未反馈负价值的 (及所有已达到末帧的canset),在actYes超时未反馈后,计SP+ `T`; |
| TODO7 | 改下把末帧反馈负价值的 (及所有已达到末帧的canset),在feedbackTOP反馈匹配后,计SP- `T`; |
| TODO8 | 把canset竞争由InSP改成由OutSP来计算 `T`; |
| 回测 | 继续上表末的试错训练: `试跑几次,饿,上扔无皮果`,看下canset的竞争情况会不会快速响应竞争变化; |
| 结果 | 经测跑三四次`饿,无皮果,飞,吃`后,是可以快速响应变化,使这些有用的canset快速具备竞争力 `存为FZ955x6`; |

**小结: 上表支持了OutSPDic,然后可以快速响应试错效率提升了,但是这改动会影响OutSP评分,所以TO阶段的训练也需要重新训练下: 下面重跑下训练FZ66,以及继续推进31183的训练项3-无皮果动机;**

***

## n32p01b 回测
`CreateTime 2024.06.21`

上节支持了OutSPDic,本节回测;

| 32013 | 在FZ955基础上接着继续31183-训练项3的`无皮果动机`及后续训练项 (参考31183) |
| --- | --- |
| 训练项1 | 测下newRCanst,absRCanset,newHCanset,absHCanset四个都能执行到 `在31183完成过,此处重训练下`; |
|  | > 步骤4: (`路边出生,饿,路上扔带皮果,扔棒去皮,手动飞至,触发吃掉`) x 路下n次 (注:鸟与果相对位置要稳定些) |
|  | > 注意: 跑前几次还好,后几次最好边观察hSolution有激活结果,边慢训几次,以使absHCanset可执行到; |
|  | > 说明: 在FZ913基础上,按此步骤4重训练,得到FZ964 `T`; |
| 训练项2 | 试错训练: 测下可以快速响应Canset的OutSPDic变化,使有用的canset快速具备竞争力; |
|  | > 步骤5: 跑三四次`饿,无皮果,飞,吃`,存为FZ965 `T`; |
| 训练项3 | 测下无皮果动机; |
|  | > 训练6 `FZ965,路下出生,点击饿` (说明: 观察日志,看能否得到无皮果动机); |
|  | > 日志: HDemand`A4584(向89,距12,果)`,`A4467(向92,距12,果)`,`A4075(向86,距12,果)`等,可见无皮果动机ok `T`; |
| 训练项4 | 学去皮: 学会去皮(压去皮) `T`; |
|  | > 训练步骤: 1.在去皮动机生成H无皮果后 2.扔有皮果 3.扔棒去皮 4.feedbackTOR反馈到无皮果 5.生成扔棒去皮H经验 |
|  | > 具体步骤: FZ965,路上出生,点击饿,在生成H无皮果后,扔有皮果,扔木棒去皮,上飞吃掉; |
|  | > 取一些训练中关键日志如下: |
|  | H无皮果日志1: `flt1 A4204(距12,果)` |
|  | H无皮果日志2: `flt1 A4592(向88,距0,果)` |
|  | H无皮果有反馈成立1: `flt2 R feedbackTOR反馈成立:A4584(向89,距12,果) 匹配:1 baseCansetFrom:F4622[↑饿-16,4果,飞↑,4果,吃] 状态:CS_Besting` |
|  | H无皮果有反馈成立2: `flt2 R feedbackTOR反馈成立:A4204(距12,果) 匹配:1 baseCansetFrom:F4565[↑饿-16,3果,飞↑,3果,吃] 状态:CS_Besting` |
|  | 学会去皮HCanset经验1: `flt3 Canset演化> AbsHCanset:F5611[M1{↑饿-16},A4204(距12,果)] toScene:F4565[↑饿-16,3果,飞↑,3果,吃] 在2_1帧:A4467(向92,距12,果)` |
|  | 学会去皮HCanset经验1: `flt3 Canset演化> NewHCanset:F5612[M1{↑饿-16}] toScene:F4622[↑饿-16,4果,飞↑,4果,吃] 在1帧:A4584(向89,距12,果)` |
|  | > 说明: 如上日志可见,此训练过程能够生成扔棒去皮H经验,并存为:FZ966 `T`; |
| 训练项5 | 用去皮: 能生成H有皮果动机; |
|  | 其间问题1: 其间发现反思不通过,加训[饿,更饿]和[饿,皮果,去皮,食之] (参考32016-方案); |
|  | 其间解决1: 具体解决的训练步骤:`饿,更饿,更饿,皮果,扔棒去皮,飞至食之` (这个可以加强饿更饿,并且减弱饿皮果更饿),存为FZ967 `T`; |
|  | 结果日志1: `> F5841行为化前 的 子任务分:-9.91 > 当前任务分(饿):-10.86 =====> 已通过`,日志可见,它可以反思通过了; |
|  | 继续回测1: 继续测有皮果动机步骤: `FZ967,饿`,效果如下: |
|  | > H有皮果动机ok: `TCDemand.m  39] A4006(向87,距12,皮果)`,日志可见,可以生成H有皮果动机了 `T`; |
|  | 其间问题2: 测得在`更饿`发生后任务会失效,导致来不及生成"有皮果动机",这个root就不再有激活机会了 |
|  | 其间解决2: 在n32p04中,已经调整"持续饿感"的"任务失效机制":调整为负mv反馈后任务不失效,此问题已修复 (参考n32p04); |
|  | 继续回测2: 继续测有皮果动机步骤: `FZ967,饿`,效果如下: |
|  | > H有皮果动机执行行为化ok: `R行为化中间帧下标 (1/10) A3955(向90,距13,皮果)` |
|  | 结果: 随着以上两个BUG的修复,H有皮果动机已经彻底ok,下面开始训练学搬运 `转训练项6`; |
| 训练项6 | 学搬运: 学会搬运(踢坚果); |
|  | 说明: 前几个训练项已经在下面直至n32p04修了许多个BUG,此处训练项6的推进转到n32p05中继续干 (转n32p05); |
| 训练项7 | 用搬运: 使用搬运; |

**小结: 上表就是本节的训练测试的主表,下面都是些中途发现BUG并修复的记录;**

| 32014 | BUG-生成canset时的indexDic又有越界问题了,查下原因 |
| --- | --- |
| 原因 | 在NewHCanset时,使用了self.realCansetToIndexDic映射,但这个映射此时也还在更新中,导致有更新后,越界了; |
| 修复 | 创建NewHCanst时先把realCansetToIndexDic.copy(),这样后续再更新,就不会同步追加到已经创建的NewHCanset映射中 `T`; |

```txt
32015: 测有皮果动机-测得反思因噎废食不通过问题
复现: `FZ966,路下出生,点击饿`
说明: 如下日志:`1.hSolution输出了有皮果的解 2.行为化前先反思并得到P20条解 3.形成了子任务 4.反思没通过`
反思: 的双方为: A任务本身是为了防止更饿 <= PK => B以往失败时它也会更饿;
    > 分析: 以往的更饿,与执行Canset无关,即使不执行,它也会变更饿;
示例1: 喝药没治好病,只能说明药没用,而不是药使你继续病 (用药的后续病重程度 ≈ 不用药的持续病重程度);
示例2: 中药用错即为毒,确实会让病更重 (用药的后续病重程度 > 不用药的持续病重程度);
示例3: 喝药非常苦,不管病好不好,药苦是药带来的;
    > 分析: 下方日志中,其实二者是约等于的,此时不应因此而`因噎废食`;
方案1: 留下50%的余地,即`A x 1.5 < B`时才判定为反思不通过 (参考3005a-方案1 已经做过);
方案2: 不应该取`最严重子任务分`,因为那个最严重也未必就会发生,所以取每个子任务的平均分 `先选定此方案`;
方案3: 不应该取`最严重子任务分`,因为那个最严重也未必就会发生,所以按每个子任务的pFo的强度来分配评分比重求出最终综合得分 `现在本就支持sp率影响评分`;
抉择: 方案2简单便捷,并且本来就支持spScore对评分的影响,暂选定,进行实践 `已实践 T`;
回测: 经回测,改完方案2后还是反思不通过,平均分依然很高,导致不通过 `转32016 T`;
3438 [11:14:52:084 TO        TCSolution.m  39] =============================== 12 hSolution ===============================
3439 [11:14:52:084 TO        TCSolution.m  39] flt2 目标:A4628(向89,距11,果) 已有S数:0
3440 [11:14:52:160 TO    TCSolutionUtil.m 117] 第2步 H转为候选集:29 - 中间帧被初始传染:9 = 有效数:20
3441 [11:14:52:161 TO    TCSolutionUtil.m 198] 第5步 HAnaylst匹配成功:29
3442 [11:14:52:161 TO    TCSolutionUtil.m 206] 第6步 H排除Status无效的:29
3443 [11:14:52:161 TO    TCSolutionUtil.m 212] 第7步 H排除Infected传染掉的:20
3444 [11:14:52:162 TO    TCSolutionUtil.m 218] 第8步 H排除FRSTime来不及的:18
3445 [11:14:52:164 TO            AIRank.m 189] flt3 H0. I<F3998 F4228[↑饿-16,4果皮]> {0 = 0;}  (null):(分:0.00) [CUT:0=>TAR:1]
3446 [11:14:52:165 TO            AIRank.m 189] flt3 H1. I<F3998 F4241[↑饿-16,4果皮]> {0 = 0;}  (null):(分:0.00) [CUT:0=>TAR:1]
3447 [11:14:52:166 TO            AIRank.m 189] flt3 H2. I<F3998 F4327[↑饿-16,4果皮,4棒]> {0 = 0;}  (null):(分:0.00) [CUT:0=>TAR:1]
3448 [11:14:52:167 TO            AIRank.m 189] flt3 H3. I<F3998 F4249[↑饿-16,4果皮]> {0 = 0;}  (null):(分:0.00) [CUT:0=>TAR:1]
3449 [11:14:52:168 TO            AIRank.m 189] flt3 H4. I<F3998 F4342[↑饿-16,4果皮,4棒]> {0 = 0;}  (null):(分:0.00) [CUT:0=>TAR:1]
3450 [11:14:52:169 TO      TCRefrection.m  39]
3451 [11:14:52:169 TO      TCRefrection.m  39]
3452 [11:14:52:169 TO      TCRefrection.m  39] =============================== 12 TCRefrection反思 ===============================
3453 [11:14:52:169 TO      TCRefrection.m  39] F4228[↑饿-16,4果皮] CUT:0 无效率:0.00
3454 [11:14:52:169 TO      TCRefrection.m  67] 反思评价结果:已通过 (解决任务奖励分10.0 Canset风险:0.00 懒分:0.0 = 10.0)
3455 [11:14:52:169 TO    TCSolutionUtil.m 256] 第9步 H求解最佳结果:F4228 {}
3456 [11:14:52:185 TO AIThinkingControl.m 271] energy > delta:-1.00 = energy:18.00
3457 [11:14:52:190 TO        TCSolution.m 278] > newH 第29例: eff: sp:{} I scene:F3998 canset:F4228 (cutIndex:0=>targetIndex:1)
3458 [11:14:52:199 TO     TCRecognition.m  35]
3459 [11:14:52:199 TO     TCRecognition.m  35]
3460 [11:14:52:199 TO     TCRecognition.m  35] =============================== 12 行为化前 反思识别 ===============================
3461 [11:14:52:199 TO     TCRecognition.m  35] regroupFo:F6332[M1{↑饿-16},M1{↑饿-16},M1{↑饿-16},M1{↑饿-16},A4227(向91,距13,皮果)]
3482 [11:14:53:443 TO          AIFilter.m 351] 过滤器: 总91需20 主:0.22 => 剩:20
3483 [11:14:53:443 TO           TIUtils.m 470]
3484 [11:14:53:443 TO           TIUtils.m 470] 时序识别结果 P(20条) R(0条)
3485 [11:14:53:633 TO          AIFilter.m 191]
3486 [11:14:53:633 TO          AIFilter.m 191] 时序二次过滤后条数: 剩3 >>>>>>>>>>>>>>>>>>>>>
3487 [11:14:53:634 TO          AIFilter.m 192] 	1. F1351[M1{↑饿-16},A1348(距47,向147,皮果)]
3488 [11:14:53:635 TO          AIFilter.m 192] 	2. F518[M1{↑饿-16},A515(距32,向72,皮果)]
3489 [11:14:53:636 TO          AIFilter.m 192] 	3. F1323[M1{↑饿-16},A1320(距63,向58,皮果)]
3490 [11:14:53:637 TO           TCDebug.m  61] [TCRecognition.m => TCDemand.m] 操作计数:811 用时:****** (616) (读:0 写:0)
3491 [11:14:53:639 TO          TCDemand.m  39]
3492 [11:14:53:639 TO          TCDemand.m  39]
3493 [11:14:53:639 TO          TCDemand.m  39] =============================== 13 subDemand ===============================
3494 [11:14:53:639 TO          TCDemand.m  39] 子任务数:1 baseFo:F6331[M1{↑饿-16},A4227(向91,距13,皮果)]
3495 [11:14:53:640 TO          TCDemand.m  76] 	 pFo:F1351[M1{↑饿-16},A1348(距47,向147,皮果)]->{饿-16.00}
3496 [11:14:53:641 TO          TCDemand.m  76] 	 pFo:F1323[M1{↑饿-16},A1320(距63,向58,皮果)]->{饿-16.00}
3497 [11:14:53:642 TO          TCDemand.m  76] 	 pFo:F518[M1{↑饿-16},A515(距32,向72,皮果)]->{饿-12.80}
3498 [11:14:53:642 TO      TCRefrection.m  39]
3499 [11:14:53:642 TO      TCRefrection.m  39]
3500 [11:14:53:642 TO      TCRefrection.m  39] =============================== 13 行为化前 反思评价 ===============================
3501 [11:14:53:643 TO      TCRefrection.m 104] > 最严重子任务分(饿):-10.45 > 当前任务分(饿):-10.04 =====> 未通过
3502 [11:14:53:643 TO AIThinkingControl.m 242] TO上轮:失败 等待:0.0 下轮:10 消息:action反思不通过
3626 [11:14:54:645 TO     DemandManager.m 300] 	第1条 饿 评分-10.17 激活成功 	{proto:F6334 pFos:(F3871,F3825,F3566,F3717,F3579,F3597,F3521,F3531,F3543,F3553,F3611,F3629,F3651,F3699,F3644,F3559,F3546,F3534,F3524)}
3627 [11:14:54:646 TO     DemandManager.m 303] Demand竞争 <<<== SUCCESS 共6条
3628 [11:14:54:647 TO            TCPlan.m  51] 取得最终胜利的末枝 >> 取分: K:0x600003f04750_ => V:(null)分
```

```txt
32016-反思子任务迫切度太高,导致反思仍不通过;
//=============================== 13 subDemand ===============================
//子任务数:1 baseFo:F6642[M1{↑饿-16},A4227(向91,距13,皮果)]
//pFo:F1351[M1{↑饿-16},A1348(距47,向147,皮果)]->{饿-16.00}
//pFo:F1323[M1{↑饿-16},A1320(距63,向58,皮果)]->{饿-16.00}
//pFo:F518[M1{↑饿-16},A515(距32,向72,皮果)]->{饿-12.80}
//=============================== 13 行为化前 反思评价 ===============================
//> 行为化前:F4228 的 子任务分:-10.45 > 当前任务分(饿):-10.04 =====> 未通过

分析1. 在经验中,场景pFo饿了不一定会更饿,所以任务分在10.x左右 `即[饿]->{更饿} 只有65%稳定性`;
分析2. 而在反思预测中,饿了看到皮果,的场景pFo,却更大几率会更饿,所以它的评分在15左右 `即[饿,皮果]->{更饿} 却有95%稳定性`;
思路: 可以把[饿]->{更饿}训练的更稳定,或者把[饿,皮果]->{更饿}训练的更不稳定,即以下两个方案都执行下来试下;
方案1: 降低子任务分: 看下多训练几下: 看到有皮果后,去皮并吃到,看能不能把这个几率降低一下;
  > 实践: 训练步骤`饥饿,皮果,扔棒去皮,飞至食之` => 使之知道皮果去皮一吃是可行不再饿;
方案2: 提升父任务分: 当饥饿多次无解,它自然会任务分更高,直到饿的程度足够它冒险尝试;
  > 实践: 训练步骤`饥饿,等一会让它多饿几次` => 使之知道稳定不解决一直会饿;
结果: `转32013-训练项5-其间问题1` 加训并存为FZ967 (最终训练步骤也转32013-其间问题1);
```

```txt
32017: 查为什么H有皮果有动机,但没持续推进求解,日志如下:
问题说明: 在`32013-训练项5`,训练有皮果动机时,发现生成了`H有皮果`,但没执行`hSolution有皮果`,经测,发现更饿发生后,`H有皮果`所在的Root再也没激活了,导致没能持续性去推进它,本节重点解决Root的持续性问题;
回顾: 以前其实已经搞过Root的持续性 (在TCDemandManager.refreshCmvCacheSort()中,根据其Canset的进度来评分) (参考31052);
//第1条 饿 评分-10.32 激活成功     {proto:F7597 pFos:(F3871,F3825,F3900,F3566,F3717,F3579,F3597,F3521,F3531,F3543,F3553,F3611,F3629,F3651,F3699,F3644,F3559,F3546,F3534,F3524)}
//第2条 饿 评分0.00             {proto:F7592 pFos:(F7304,F7061,F3871,F3825,F3900,F5594,F7175,F3566,F3717,F3579,F3597,F3521,F3531,F3543,F3553,F3611,F3629,F3651,F3699,F3908)}
日志说明: 如上日志,可见,第1条是更饿,第2条并不是没持续性,而是它评分为0,导致它没激活;
1. 查下此处,前一个饿任务明明还在推进"H有皮果",为什么会被评分0分 (所有pFo都无效);
 分析1. 先查下此处挂0的原因: 为什么都无效了,如果有任务在推进中,但是个持续性任务怎么办?新root能快速复用到旧root的成果吗?
2. 然后看下,以前写的任务推进的持续性,能不能有效,帮助他持续推进下去...
 分析2. 以前为保证任务持续性,对root的竞争显然无效,应该在root推进了几层树时,相应的就加几层权,使之可持续;
修复: 在refreshCmvCacheSort()中改为调用score4Demand_Out()后,它不再是0分了,日志:`root(0/2):F7821[M1{↑饿-16}] 评分:10.86`;
回测: 它确实不再是0分了,但仍然没激活,因为它已经是isExpired状态,导致更饿发生后,就不会再激活了,它的一切工作记忆的推进进度也都白白浪费了 `此问题转n32p04`;
```

```txt
3201x-接31184结果: 无皮果动机和有皮果动机,都需要加训和试错;
//1. 加训无皮果RCanset: 路下出生,扔路上无皮果,上飞至,吃掉;
//      > 目标: 使之能稳定的激活上面习得的rCanset[无皮果,飞至,吃掉],然后生成无皮果hDemand;
//2. 加训有皮果HCanset: 生成无皮果HDemand后,扔有皮果,扔木棒去皮,h无皮果反馈成立,生成无皮果hCanset;
//      > 目标: 使之能稳定的激活上面的hCanset[有皮果,木棒,无皮果],然后生成有皮果的hDemand;
//3. 试错训练: 然后试下,这么多各种canset,是否应该多淌一淌,把一条可行的路慢慢淌出来;

明日: 别着急,一步步来,先看下训练项3,无皮果动机是哪来的,是不是rCanset[无皮果,飞至,吃];
结果: 这个表后来没怎么用,因为后来用(无皮果父任务,有皮果子任务,有皮果搬运子子任务 => 然后先搬运,再压破皮,再飞过去吃),这套流程来一节节推进的 (参考下面几节的手稿记录);
```

**小结: 上面针对31183中,训练不顺的那些训练项,制定了更为细节的训练规划;**

TODO测试项3: 测试RCanset有皮果反馈后,能不能继续推后进下一帧;

***

## n32p02 多层子H任务嵌套时的H迁移
`CreateTime 2024.06.09`

起因: 在上节最后的测试中,发现多层H嵌套时,H的迁移应该只支持一层 (测得不支持多层H迁移问题);
1. 原先写的是H的scene必然是rCanset
2. 但其实H的scene可能是baseH,不一定就是rCanset;

示例: H任务在工作记忆中是会多层嵌套的,比如:
1. 无皮果的HCanset为[有皮果,压,无皮果]
2. 然后它的有皮果又有子H任务[路边有皮果,踢,路中有皮果]

问题: 以前H的迁移比R多一层,在这里来看,这是不对的,H不止是一层,它可能多出多层 (因为子H可能嵌套多层);
1. 本节主要分析并支持这种多层下H迁移的情况;

| 32021 | 回顾现有代码之: 多层hCanset合并为单层rCanset; |
| --- | --- |
| 示图 | ![](assets/720_多层HCanset生成为单层RCanset.png) |
| 说明 | 如图,H确实会嵌套,但它的成果会被打包成一个(合并)整体的rCanset; |
| 总结 | 多层H的成果确实是一个rCanset,但成果之前,它确实是多个h嵌套 (即`本表不妨碍本节的问题依然存在`); |

| 32022 | 回顾现有代码之: h迁移也是基于rScene树的; |
| --- | --- |
| 说明 | 现有代码中,H的迁移也是基于RScene树来进行的,所以H也许不会有多层嵌套的问题; |
| 解释 | 即无论H有多少层嵌套,它在长时记忆中都是场景树,比R多一层而已; |
| 结果 | 本表可见,本节多层子H嵌套的问题压根不存在; |

**小结: 从32022的结果可见,本节的问题应该是不存在的,中止之;**

***

## n32p03 整理下：HE模型流程图
`CreateTime 2024.06.15`

| 32031 | HE流程图 |
| --- | --- |
| 彩图 | ![](assets/722_HE模型流程图.png) |
| 说明 | https://www.bilibili.com/video/BV1Xw4m1e7RH/ |
| 注1 | 反馈由外循环推动(现实与智能体循环),意向性推动了内循环(认知与决策循环); |
| 注2 | 识别与求解相对,识别是IN阶段的展开,求解是OUT阶段的展开; |
| 注3 | 此图临时起意,细节上用的名词等并不严谨,但与HE代码的流程是一致的,不影响"表达和理解"其意; |

**小结: 最近试用了一下B站直播功能,分享了HE流程图,顺便把以上模型图画了下;**

***

## n32p04 调整"持续饿感"的"任务失效机制" : 调整为负mv反馈后任务不失效
`CreateTime 2024.07.03`

说明: 在`32017-回测`中,测得连续饿感在任务失效后,也没法激活,但其实它是连续饿感,并且还没解决,应该能`继续推进任务的解决`才对,本节针对此问题做改动与支持;
示例: 当饥饿任务还在解决中,就发生了更饿时,此时原饥饿任务应该继续解决,而不是直接计为isExpired状态就白白浪费掉;

| 32041 | 分析方案 |
| --- | --- |
| 分析 | 饥饿是持续性任务,疼痛不是,比如婴儿已经饿过了,还在找吃的,要不要标一下持续性 (把持续饥饿搞的更深入一些,而不止是隔时间就触发下)? |
| 思路 | 持续饿感,在更饿发生后,说明它还没解决,正应该继续推进解决,而不是失效中断之; |
| 方案 | 根据mv类型,转为是否为持续mv,如果是持续的,那么只要未解决,就永远不会失效(isExpired); |
| TODO1 | 写个方法,用于判断mv为持续性还是单发 (如:饿感为持续性,痛感为单发) `T`; |
| TODO2 | pFo发生负价值反馈时: 单发价值=>已不可挽回计为失效 & 持续价值感=>发生后还会再发生不计失效 `T`; |
| TODO3 | 持续价值的pFo在到期后,发现已经有mv反馈(但它发生后还会发生),所以不可计为失效状态 (反之,一直没反馈,说明已经自然完成,可以计为失效) `T`; |
| 回测 | 第1条 饿 评分-10.86 激活成功 	{proto:F7926 pFos:(F7304,F7061,F3871,F3825,F3900,F5594,F7175,F3566,F3717,F3579,F3597,F3521,F3531,F3543,F3553,F3611,F3629,F3651,F3699,F3908)} |
| 结果 | 根据以上回测日志可见,原饥饿root可以顺利激活了 `T`; |

**小结: 上表为本节主体,改后回测已ok**

```txt
32042-测得`有皮果动机行为化前的反思`又不通过的问题 (参考32016)
说明: 如下,测得新的问题,虽然激活了,但它反思不通过,
//1. 它已经可以激活有皮果动机了,如下日志;
R0. I<F3825 F3986[↑饿-16,4果皮,4棒,4棒,4果,飞↑,4棒,4果,4棒,吃]> {0 = 0;} {9 = S0P16;5 = S0P19;1 = S1P20;10 = S0P2;6 = S0P16;2 = S0P19;7 = S0P16;3 = S0P19;8 = S0P16;4 = S0P19;} H3N2:(分:0.60) [CUT:0=>TAR:10]

//2. 但反思又失败了,看起来像是: 前期训练学认有皮果时跑了上百轮,有了太稳定会饿的副作用;
=============================== 5 subDemand ===============================
子任务数:1 baseFo:F3986[M1{↑饿-16},A3955(向90,距13,皮果),A3958(距89,向18,棒),A3961(向87,距27,棒),A3962(向90,距13,果),飞↑,A3970(向164,距71,棒),A3971(向90,距5,果),A3976(距73,向166,棒),A3979(吃1)]
	 pFo:F200[M1{↑饿-16},A197(向136,距101,皮果)]->{饿-16.00}
	 pFo:F387[M1{↑饿-16},A384(向23,距67,皮果)]->{饿-16.00}
	 pFo:F1351[M1{↑饿-16},A1348(距47,向147,皮果)]->{饿-16.00}
=============================== 5 行为化前 反思评价 ===============================
> F3986行为化前 的 子任务分:-11.20 > 当前任务分(饿):-10.86 =====> 未通过

方案1: 可以试下32016的解决方法应该还是有效果的;
  > 试了下,这方法倒是管用,就是跑着比较慢,毕竟在当时`30092-步骤1`时,对有皮果跑了200次,这副作用有点大,形成阴影了,看到皮果就认定了会更饿;
  > 所以: 这方法虽然管用,但有点麻烦,暂不考虑了;
方案2: 在反思时,可以把任务求解的进度分考虑入内,毕竟在DemandManager中root竞争时就用了进度总分;
  > 此方案简单可行也合理 `95% 选定`;
TODO1: 把原来refreshCmvCacheSort()中求含进度的任务总分,封装成一个方法(封装成了progressScore4Demand_Out)来计算含进度的任务分 `T`;
TODO2: 在actionRefrection行为化反思中,把任务分改为调用"含进度的任务总分",这样应该稳稳的反思能通过了 `T`;
回测日志如下:
日志1. > F3986行为化前 的 子任务分:-11.20 > 当前任务分(饿):-16.49 =====> 已通过
日志2. =============================== 5 行为化Fo ===============================
日志3. R行为化中间帧下标 (1/10) A3955(向90,距13,皮果)
回测说明: 经回测,方案2实践后,有皮果动机反思可以顺利通过了,主要是含了进度影响后,任务总分更迫切了 `T`;
```

**小结: 上表是个行为化前反思不通过的BUG,调整任务为加上进度分影响后,ok了;**

***

## n32p05 学搬运
`CreateTime 2024.07.05`

本节接续`32013的训练项6学搬运`,把训练中遇到的问题或细节等在本节处理修复等;

| 32051 | 训练: 学搬运 (参考32013-训练项6) |
| --- | --- |
| 训练项6 | 学搬运: 学会搬运 (学会踢坚果能使坚果位置变化); |
| 步骤 | `FZ967,饿,等看到有皮果动机后,扔路边带皮果,踢到路上` x 5次; |
| 重点日志1 | `flt1 A4006(向87,距12,皮果)` (有皮果动机); |
| 重点日志2 | `flt2 Canset演化> NewHCanset:F7772[M1{↑饿-16},A7764(向190,距0,皮果),踢↑,A7771(向94,距9,皮果)] toScene:F3986[↑饿-16,4果皮,4棒,4棒,4果,飞↑,4棒,4果,4棒,吃] 在1帧:A3955(向90,距13,皮果)` (学会搬运); |
| 注 | 步骤执行中,如果点击饿后,半天没皮果动机,那多等一会应该就有了 (它需要canset池自行做试错); |
| 结果 | 将以上训练跑完后 `T 存为FZ968`; |

***

## n32p06 用搬运（一）
`CreateTime 2024.07.06`

本节接续`32013的训练项7用搬运`,把训练中遇到的问题或细节等在本节处理修复等;

| 32061 | 训练: 用搬运 (参考32013-训练项7) |
| --- | --- |
| 训练项7 | 用搬运: 使用搬运 (主动有目的的踢坚果); |
| 步骤 | `FZ968,饿,等看到距0有皮果动机后,扔距0带皮果` -> 然后看它自己把坚果踢到路上; |

| 32062 | 测到BUG: 有皮果的hDemand生成后,70%执行不到hSolution的问题 |
| --- | --- |
| 调试 | 经调试,hSolution没能继续执行它的原因有好几种: |
|  | 第1种情况: 有时候是hDemand在TCPlan时,已经ActNo掉了,导致激活不到 |
|  | 第2种情况: 有时候是TCPlan中,hDemand的baseRDemand同级竞争后,在TCScore中都是0分,导致二者执行到哪个随缘; |
| 结果 | 干脆把TCPlan迭代下V2,它很久没迭代了,其实老以前的做法太老旧不适用了 `转下节`; |
| 追加 | 后来在迭代TCPlanV2后回测此BUG好了 `参考32073-回测`; |

**总结：用搬运训练暂停，等下节把TCPlanV2写后，在下下节继续搞“用搬运”。**

***

## n32p07 迭代TCPlanV2
`CreateTime 2024.07.11`

TCPlan已经很久没迭代了,它当时的竞争设计,放在现在其实明显不足了:
1. 旧: 前几天尝试打日志观察它的竞争过程,原来是纯粹的"综合评分"来竞争的;
2. 新: 但现在像Canset池是支持实时竞争的 (我们用不着它那么复杂的去掉综合评分);
所以本节,思考一下,是否直接迭代下TCPlanV2,看重新设计一下它的竞争机制;

| 32071 | 初步规划迭代 |
| --- | --- |
| 分析 | 工作记忆中,大概就三种分枝: Canset池,子R任务,子H任务; |
| 1 | canset池有实时竞争功能,不需要TCScore再做综合评分; |
| 2 | 子r任务间有进度分排序,也不需要综合评分做竞争; |
| 3 | 子h任务在推进中的alg只一条,不需要竞争; |
| 问题1 | 子R任务的负分仍然存在,用不用考虑到这些,否掉呢? (就像反思不通过,只是此处针对工作记忆整树来综合评定); |
|  | > 如果这条有必要,那么还是需要综合评分; |
|  | > 当然不需要每一条都综合评分了,而是把best做"整树反思"不通过(类似反思评价),即可; |
| 问题2 | 子R任务用不用优先解决掉它, 比如:翻山前提前准备好防虎枪 `T`; |
|  | > 感觉这种复杂的设想,其实借助最简单的Scene->Canset结构也是可解的; |
|  | > 现在倒不必想这些,兴许到时候`危险`做为另外一个root,它的解就是准备好打虎枪; |
|  | > 而原本的翻山任务,和危险任务,是两个root,压根不需要混合的做TCPlan规划一个解出来 (把事想复杂,往往难控制); |
|  | > 思路: 1. 比如,我们把子R任务另起成一个Root,当它很强时(比如准备枪防止危险),自然会战胜它的源Root(翻山); |
|  | >      2. 而这个另起的Root被解决后,源Root中自然也会因为`传染唤醒`机制,而变的不同(即不再有预测的危险); |
|  | > 所以: 这一条先不考虑,以后再支持,我们不优先解决子R任务 `T`; |
| 改动3 | 子任务(分枝叶)中有Finish或ActNo或传染掉时，不可激活。 |
|  | > 介入示例思考下,怎样让它又不激活无解的问题,又能不影响到像`搬运坚果超时`时的Canset推进; |
|  | > 比如: 没钱时,不会再去买饭 & 步行去目的地超时了,仍会继续走下去; |
| 总结 | 1. 本表通过近年来决策的变化,分析了3条TCPlanV1已经落后了的证据; |
|  | 2. 然后通过`问题2`跳过了子R任务,又把TCPlan简化了下; |
| 结果 | 下一步,TCPlanV2的模型和实践转下表 `T`; |

| 32072 | 模型图和代码实践 |
| --- | --- |
| 模型 | ![](assets/723_TCPlanV2模型图.png) |
| 说明 | 如上图,TCPlanV2是一个Root加多个Canset的结构,递归时,也是顺着这个结构一级级向回走; |
|  | 1. 如果单条失败就尝试下一条,找下一个baseDemand.bestCanset (重新实时竞争,得出最佳解); |
|  | 2. 如果单条成功就继续下一层,找下一个subDemand.bestCanset (继续实时竞争,得出最佳解); |
|  | 3. 如果全部失败就退至上一层,找上一个otherBaseDemand (重新实时竞争,得出另一个最佳解); |
| 实践 | 结构较为简单,代码也清晰,所以直接看以上模型图和TCPlanV2的代码即可 `T`; |
| 改进点1 | 当子枝叶全失败后,而当前bestFo是ActYes状态,才静默等待 (比如: 等饭熟,有苹果也会先吃一个垫垫); |

```txt
32073-回测: 如下,经测每次生成hDemand后(A8616),立马在TCPlanV2后,可以执行到它的solution() `此问题参考32062`;
=============================== 9 hDemand ===============================
flt1 A8616(向140,距0,果)
TO上轮:成功 等待:0.0 下轮:7 消息:out 此帧需要HDemand来完成,已转为h任务
---------------------------------------------------------- START
* itemRoot -> 执行:F10124[↑饿-16]
  > item评分cansets竞争 -> 胜者:F3994[M1{↑饿-16},A3955(向90,距13,皮果),A3958(距89,向18,棒),A3961(向87,距27,棒),A3962(向90,距13,果),飞↑,A3970(向164,距71,棒),A3971(向90,距5,果),A3976(距73,向166,棒),A3979(吃1)]
    - itemHDemand -> 执行:A3955(4果皮)
      # item评分cansets竞争 -> 胜者:F8632[M1{↑饿-16},M1{↑饿-16},A8616(向140,距0,果),A3979(吃1),A8624(向334,距0,皮果),踢↑,A8629(向85,距9,皮果)]
        ⊙ itemHDemand -> 执行:A8616(4果)
planV2 success3 执行subHDemand的求解:A8616(4果)
---------------------------------------------------------- FINISH
```

***

## n32p08 用搬运（二）
`CreateTime 2024.07.13`

在上上节中，测用搬运最终被TCPlanV2阻搁了，本节继续搞这一训练。

| 32081 | 训练: 用搬运 (参考32013-训练项7) |
| --- | --- |
| 训练项7 | 用搬运: 使用搬运 (主动有目的的踢坚果); |
| 步骤 | `FZ968,饿,等看到距0有皮果动机后,扔距0带皮果` -> 然后看它自己把坚果踢到路上; |

| 32082 | 场景能提供的条件: 来引导推进测试 |
| --- | --- |
| 问题 | 无皮果动机,原本无皮果所在的Canset是能优胜的,但后来随着别的训练后,又没法优胜了(它其间又有几次S发生了); |
| 分析 | 从`无皮果动机`->`有皮果动机`->`距0有皮果动机`,难道智能体正好依次让这几个所在的Canset优胜吗? |
| 思路 | 这三步,必然没法都优胜,这种理想化情况是不现实的; |
|  | 那么: 有没有另外一种场景下,另外的几步也可行呢?那么难道我们也要让B场景下的这几步依次优胜?这也不现实; |
| 线索 | 说白了,各种场景下,能让各自能优胜的情况是不同的,即关键在于`各场景下能提供的外在条件不同`; |
| 方案 | 那么,让场景下能提供什么来决定即可 (比如: 并不是带皮果能优胜,而是我们这个场景有带皮果提供给智能体); |
|  | 说明: 我们不需要改代码,原本Canste池就是支持实时反馈的,并且随着实时反馈,实时竞争也能实时切换优胜Canset; |
| 实践 | 训练用搬运时: 也许不需要激活距0带皮果,只要它在canset池中了,就可以扔一个,帮助他在实时竞争中胜出; |
| 结果 | 经测试,这是有效的,见`20240715-把坚果踢到路上让压破皮.mp4` |

| 32083 | 用搬运不稳定的问题 (一) |
| --- | --- |
| 说明 | 在上表中,把搬运跑通了(但不稳定,有时它想不到搬运,有时能想到),本表查下原因; |
| 分析 | 通过以下代码段1到代码段4,来分析此问题的原因,分析结果转32084继续; |

```java
32083-代码段1: 通过观察日志看能不能看出有什么问题;
=============================== 3 hSolution ===============================
目标:A3955(向90,距13,皮果) 已有S数:0
H0. I<F3991 F10282[4棒,吃,踢↑,↑饿-16,4果皮,4果皮]> {0 = 3;1 = 5;}  (null):(分:0.00) [CUT:3=>TAR:5] //这里输出的只有一例;

=============================== 3 行为化Fo ===============================
H行为化中间帧下标 (4/5) A10269(向110,距0,皮果) from时序:F10439[A10209(向172,距131,棒),A3979(吃1),踢↑,M1{↑饿-16},A10269(向110,距0,皮果),A10269(向110,距0,皮果)]

=============================== 4 hDemand ===============================
flt1 A10269(向110,距0,皮果)

=============================== 5 hSolution ===============================
目标:A10269(向110,距0,皮果) 已有S数:0
>>>>>> hSolution 无计可施

1疑点: 这里生成子H是不是应该限一下层级,总不能不断循环吧,不断循环下去,子H也变味了;
1解答: 不必理这个,实测了下,一般也就循环最多两三层;

2疑问: 并且当时记得H迁移是依赖RSceneTree的,那么是不是导致了?它迁移不了?或者多层后,还能迁移输出的结果很少?
2解答: 实测一般H有一两层嵌套,但hSolution输出才0到2条结果,H解太少了,但它的迁移应该是在正常工作的(不然一条都不会有);

3疑问: 经调试发现,现在这些hDemand的root都是: [饿]->{更饿},其实在期间已经扔了有皮坚果,场景已经有了变化,但它还是在跑这个root;
3分析: 现在root的持续性很强,持续root是不会因更饿的发生而过期失效的;
3方案1: 试下当旧Root全被传染时,使之彻底跪掉,换下一个新Root上继续求解;
      > 方案1线索: 所以要分析下,在理性上全被传染后,看要不要把这个root彻底跪掉,而不是总在它上面尝试解决整个饥饿问题;
      > 方案1思路: 其实场景早就变化了,应该过渡到新R场景下,来尝试求解,应该更能找到准确些的解 (场景匹配度高,是解决方案准确的前提);
      > 方案1举例: 起初饿了是想做饭来的,但看到冰箱有剩饭其实只要加热就可以;
      >>> 此时: 我们想的不再是饿了怎么加热,而是在家里冰箱边饿了时有剩饭怎么加热;
      >>> 所以: 在任务的推进过程中,R场景的及时更新,也会有助于找到更切合场景的解;

3方案2: 其实[饿]->{更饿},只是真的没解,也许训练经历一次,就可以有解的啦
      > 方案2: 正据: 刚改了TCPlanV2,也许之前的训练Canset全挂在别的Root下了,只是改了TCPlanV2后,它只在[饿]->{更饿}下在求解而已;

3结果: 经以下几个代码段实测分析,总是用[饿]->{更饿}并不影响推进解决任务 `此处不算问题`;
      > 至于场景的变化,及时同步到canset推进反馈中即可,对Root的变化没啥要求;
```

```java
32083-代码段2: 继续观察日志,收集`用搬运`不稳定的线索;

任务分:-11.17 + 最终进度分:-0.00 = 总分:-11.17 	 任务:F11175[M1{↑饿-16}]
任务分:-11.17 + 最终进度分:-5.79 = 总分:-16.96 	 任务:F11175[M1{↑饿-16}]
任务分:-11.17 + 最终进度分:-8.58 = 总分:-19.75 	 任务:F11175[M1{↑饿-16}]
任务分:-11.17 + 最终进度分:-0.00 = 总分:-11.17 	 任务:F11175[M1{↑饿-16}]
//说明: 如上日志,F11175太容易战胜别的任务了:
//  > 一来它有时有进度分,它必胜;
//  > 二来它即使没进度分,它一身评分11.17,也是最高分;

=============================== 28 hDemand ===============================
flt1 A8187(向213,距16,皮果)
TO上轮:成功 等待:0.0 下轮:15 消息:out 此帧需要HDemand来完成,已转为h任务
任务分:-11.17 + 最终进度分:-0.00 = 总分:-11.17 	 任务:F11175[M1{↑饿-16}]
任务分:-11.13 + 最终进度分:-0.00 = 总分:-11.13 	 任务:F11219[A11181(向85,距0,皮果),M1{↑饿-16},A11181(向85,距0,皮果),M1{↑饿-16}]
任务分:-9.72 + 最终进度分:-0.00 = 总分:-9.72 	 任务:F11184[M1{↑饿-16},M1{↑饿-16},A11181(向85,距0,皮果)]
任务分:-11.17 + 最终进度分:-0.00 = 总分:-11.17 	 任务:F11225[A11181(向85,距0,皮果),M1{↑饿-16},A11181(向85,距0,皮果),M1{↑饿-16}]
* itemRoot -> 执行:F11225[4果皮,↑饿-16,4果皮,↑饿-16]

//说明: 如上日志,多次发生饥饿,只有F11225侥幸胜利了,别的全败于F11175马下;

=============================== 28 rSolution ===============================
任务源:饿 protoFo:F11225[A11181(向85,距0,皮果),M1{↑饿-16},A11181(向85,距0,皮果),M1{↑饿-16}] 已有方案数:0 任务分:-11.17
R0. I<F3566 F3984[↑饿-16,4果皮,4棒,4棒,4果,飞↑,4棒,4果,4棒,吃]> {0 = 0;} {9 = S0P16;5 = S0P19;1 = S3P20;10 = S0P2;6 = S0P16;2 = S0P19;7 = S0P16;3 = S0P19;8 = S0P16;4 = S0P19;} H4N2:(分:0.67) [CUT:0=>TAR:10]

//说明: 如上日志,F11225战胜后,执行了rSolution,并且输出F3984的Canset结果;

=============================== 29 行为化Fo ===============================
R行为化中间帧下标 (1/10) A3955(向90,距13,皮果) from时序:F3984[M1{↑饿-16},A3955(向90,距13,皮果),A3958(距89,向18,棒),A3961(向87,距27,棒),A3962(向90,距13,果),飞↑,A3970(向164,距71,棒),A3971(向90,距5,果),A3976(距73,向166,棒),A3979(吃1)]

=============================== 30 hDemand ===============================
flt1 A3955(向90,距13,皮果)
TO上轮:成功 等待:0.0 下轮:16 消息:out 此帧需要HDemand来完成,已转为h任务

//说明: 如上日志,想得到距13皮果,用场景中现有的距0皮果去`踢搬运`即可;
```

```java
32083-代码段3: 继续观察日志,收集`用搬运`不稳定的线索;
* itemRoot -> 执行:F11175[↑饿-16]

//说明: 如上日志,F11175战胜了别的Root;

=============================== 21 hSolution ===============================
目标:A3955(向90,距13,皮果) 已有S数:0 任务层级:2
取HCanset候选集: 从hScene:F4119(I) 的在4帧开始取,取得HCanset数:2/2
	(F8236[↑饿-16,4果皮,4果皮,踢↑,4果皮,4棒,4棒,4果],F8250[↑饿-16,4果皮,4果皮,踢↑,4果皮,4棒,4棒,4果,飞↑])
	 item场景(I):F3629[M1{↑饿-16}] 取得候选数:0
fltx1 此H的sub到root结构:
︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹︹
ReasonDemandModel: F11175[M1{↑饿-16}]-> (普 | ActYes)
   ↑
TOFoModel: F3987[M1{↑饿-16},A3955(向90,距13,皮果),A3958(距89,向18,棒),A3961(向87,距27,棒),A3962(向90,距13,果),飞↑,A3970(向164,距71,棒),A3971(向90,距5,果),A3976(距73,向166,棒),A3979(吃1)]-> (普 | ActYes) 唤醒
   ↑
TOAlgModel: A3955(向90,距13,皮果) (普 | ActYes)
   ↑
HDemandModel
︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺︺
第2步 H转为候选集:2 - 中间帧被初始传染:0 = 有效数:2
第5步 HAnaylst匹配成功:2
第6步 H排除Status无效的:2
第7步 H排除Infected传染掉的:2
H0. I<F4119 F8236[↑饿-16,4果皮,4果皮,踢↑,4果皮,4棒,4棒,4果]> {0 = 0;} {1 = S2P0;} (null):(分:0.00) [CUT:0=>TAR:7]
H1. I<F4119 F8250[↑饿-16,4果皮,4果皮,踢↑,4果皮,4棒,4棒,4果,飞↑]> {0 = 0;} {1 = S1P0;} (null):(分:0.00) [CUT:0=>TAR:7]

//说明: 如上日志,但它的Canset最终也输出H皮果了,并且求到了HCanset踢皮果的解 (不过CUT=0,TAR=7);

=============================== 23 行为化前 反思评价 ===============================
任务分:-11.17 + 最终进度分:-8.58 = 总分:-19.75 	 任务:F11175[M1{↑饿-16}]
> F8236行为化前 的 子任务分:-7.65 > 当前任务分(饿):-19.75 =====> 已通过

=============================== 23 行为化Fo ===============================
H行为化中间帧下标 (1/7) A8187(向213,距16,皮果) from时序:F11216[M1{↑饿-16},A8187(向213,距16,皮果),A8191(向192,距0,皮果),踢↑,A8196(向107,距9,皮果),A8202(距87,向18,棒),A8214(距26,向93,棒),A8215(向107,距9,果)]

//说明: 如上日志,它轮到行为化A8187有皮果了: 这里关键在于,没有干干净净的[距0皮果,踢,距13皮果];
//线索: F11175总是胜利,应该不是问题,但没有生成抽象的踢经验,才是问题;

```

```java
32083-代码段4: 通过H任务的生成,观察日志变化;
1. flt1 A3955(向90,距13,皮果)
2. flt1 A10269(向110,距0,皮果)
3. flt1 A3958(距89,向18,棒)
4. flt1 A3955(向90,距13,皮果)//这个和第1个循环了;
5. flt1 A8187(向213,距16,皮果)
6. flt1 A3958(距89,向18,棒)
7. flt1 A8187(向213,距16,皮果)//这个和第5个循环了;
8. flt1 A3955(向90,距13,皮果)//这个和第1个,第4个循环了;

//说明: 如上日志,感觉它左突右试,但因为Canset太具象,导致总是难反馈到;
//1. 一是想要的,与现在能给的,要匹配上 (比如想要距0皮果,立马就能反馈到,现在场景里有);
//2. 二是要多生成absCanset,这样排除杂项后,才能干净利落的推进Canset解决任务;
//3. 如上日志,许多H任务是有循环的,应该避免循环这种无意义的增加层;
```

| 32084 | 用搬运不稳定的问题 (二) |
| --- | --- |
| 简介 | 从以上几个代码段来分析,大致就是各种混乱收束的不够,导致`用搬运`能不能顺利跑起来,非常不稳定; |
| 说明 | 经过上表代码段1-代码段4的分析,将分析结果总结到此表: |
| 1 | Canset不够抽象,杂项较多,导致左突右破,难以成功; |
|  | 分析: 这个看起来需要长时间积累,短时间很难把杂项剔除干净; |
|  | 思路: 但我们不能总指挥它,即使前期也应当能自己试错后跑通,然后随着跑通的次数变多,自行剔除掉杂项; |
|  | 解答: 即前期经验本来就有杂项,允许它多想多试错即可,而通过多试错后 => 可以抽象剔除杂项,并且不再需要想太多,精准直击目标; |
| 2 | 场景中即使有坚果,也无法及时反馈到Canset中 (视觉不连续); |
| 3 | H一直在嵌套了好多层,导致有重复H求解的情况 (同一个H目标,套娃求解) (参考代码段4) `T`; |
|  | 比如: 比如找奶油做蛋糕,又找蛋糕取上面的奶油; |
|  | 解答: 在这种有循环的情况下,应该打断H嵌套 (对于重复的,不必生成子H任务,实时竞争里直接过滤掉,不做为bestCanset输出); |
|  | 实践: 可以考虑到realTimeCansets实时竞争中,把curFrame已经有baseHDemand重复的,直接pass过滤掉 `T`; |

***

## n32p09 "搬运,去皮,飞至,吃掉": 连起来跑通
`CreateTime 2024.07.15`

***

## n32pxx 连续视觉
`CreateTime 2024.07.16`

在n32p08时,测得: `视觉提供`和`视觉需要`之间的矛盾:
说明: 即我前一秒触发看到了坚果,后一秒TO思维才需要坚果,我面前明明有坚果,但反馈不上
1. 现在场景的输入,和思维的速度,是不同步的,
2. 它应该会有先后问题, 即: 提前看到了,后面才需要这个反馈的;
3. 即使有: 传染机制,这个应该也有问题 (它就是在需要时反馈不到);
结果: 先解决32083的问题,完后看下`用搬运`训练是否顺利,如果不顺利,或者遇到任何确实需要连续视觉的地方,此处再继续;
注: 其实连续视觉这事提过许多次了,但需求都没有明确下来,它往往是看起来需要,但不做也没绝对的影响到训练无法推进,所以一直没搞;

***

## n32pxx TODO备忘
`CreateTime 2024.07.07`

1. 2024.07.07: 以瞬时记忆做初始唤醒 (这个可能不需要支持,因为以前的前段已经有瞬时的支持了,那么这个前段必然会出现在工作记忆中,所以它也是可以做为初始唤醒作用的,而工作记忆中的初始唤醒现在就是支持的);

<br><br><br><br><br>
