# 训练和修细节

***

<!-- TOC -->

- [训练和修细节](#训练和修细节)
  - [n32p01 试错效率低下问题: 新增OutSPDic](#n32p01-试错效率低下问题-新增outspdic)
  - [n32p01b 回测](#n32p01b-回测)
  - [n32p02 多层子H任务嵌套时的H迁移](#n32p02-多层子h任务嵌套时的h迁移)
  - [n32p03 整理下：HE模型流程图](#n32p03-整理下he模型流程图)
  - [n32p04 调整"持续饿感"的"任务失效机制" : 调整为负mv反馈后任务不失效](#n32p04-调整持续饿感的任务失效机制--调整为负mv反馈后任务不失效)
  - [n32p05 学搬运](#n32p05-学搬运)
  - [n32p06 用搬运](#n32p06-用搬运)
  - [n32pxx TODO备忘](#n32pxx-todo备忘)

<!-- /TOC -->

***

## n32p01 试错效率低下问题: 新增OutSPDic
`CreateTime 2024.06.09`

```txt
32011-我特意加训了rCanset[饿,扔无皮果,飞至,吃],但rSolution就是激活不到它;
训练: 在FZ954x7基础上: 加训饿果飞吃三次后,存为:FZ955x3;
说明: 如下日志,rSolution激活到的,都是一些[硬扛],或者[饿,果,果],这些rCanset;
3. I<F4900 F8772[M1{↑饿-16},M1{↑饿-16},M1{↑饿-16},M1{↑饿-16}]> {0 = 3;} {} (null):(分:0.00)
4. I<F4900 F8798[M1{↑饿-16},M1{↑饿-16},M1{↑饿-16},M1{↑饿-16}]> {0 = 3;} {} (null):(分:0.00)
3. I<F3611 F6351[A13(饿16,7),A4899(距11,果),A4899(距11,果)]> {} {0 = S4P2;1 = S0P1;2 = S0P1;} H2N6:(分:0.25)
4. I<F3521 F5100[A13(饿16,7),A5099(向90,果),A5099(向90,果)]> {} {0 = S0P1;1 = S1P2;2 = S3P2;} H3N3:(分:0.50)
4. I<F3521 F5100[A13(饿16,7),A5099(向90,果),A5099(向90,果)]> {} {0 = S0P1;1 = S1P2;2 = S3P2;} H3N3:(分:0.50)
解决思路: 说白了,无论是错误还是正确的rCanset,都没有积累过SPEFF,导致对的没出头,错的又易胜出,所以搞下试错训练自然就好了,如下:
试错训练: `FZ954x7,饿,上方扔无皮果`,如果它激活硬扛等错误rCanset就会败北,如果它激活[无皮果,飞,吃]就能成功解;
具体实行: 打出flt日志,step1=激活行为化的rCanset,step2=feedbackTOR反馈,step2b=feedbackTOP反馈,step3a=OR反省,step3b=OP反省,从这5个日志观察整个rCanset从激活到最终SP反馈;
遇到问题: 在跑以上试错训练时,发现试错训练有点慢,虽然传染了,但只要SP不打负分,就相当于每次遇到任务时,都得重新来一遍 `此问题转32012`;
结果: 在32012写了OutSPDic后,Canset可以快速响应了,试错效率没啥问题了 `T 参考32012-结果`;
```

**小结: 上表在做试错训练,但并不顺利,问题如下 (此问题转下表解决,待解决后,再回来继续进行试错训练);**  

**问题: 因为SPEFF仅针对cs_besting的canset,导致那些cs_none状态的,在下次重启时又可以卷土重来,而真正有用的canset[无皮果,飞,吃]则很难有出头之日 (即试错效率太低);**

| 32012 | Canset池SPEFF试错效率太低 |
| --- | --- |
| 说明 | 见上表试错训练时-遇到问题: 传染只发生在工作记忆中,长时记忆的SPEFF又仅针对转实后的Canset,导致效率低下; |
| 方案 | 即使不转实,也可以累积SPEFF,这样可以从根本上解决试错效率低下的问题; |
| 实践 | 可以在sceneTo下,直接针对sceneFrom和cansetFrom记录SPDic,这样性能才ok,可不转实就批量记录SP参数; |
|  | > 借此机会把IN和OUT的SPDic分开 (IN是对scene存SP,OUT是对canset存SP); |
| TODO1 | 新写一个AIOutSPStrong(),把sceneFrom和cansetFrom存里面,然后把spDic也存里面 `T`; |
| TODO2 | 在AIFoNodeBase里加一个outSPDic<K=sceneFromPId_cansetFromPId, V=AIOutSPStrong> `T`; |
| TODO3 | 改下在生成xvModel前(更不需要等siModel),就把outSPDic初始一下 `T`; |
|  | 时机: 在构建canset到canset池时,把cansetFrom的spDic做为初始化outSPDic (加上防重,仅初始化一次) `T` |
|  | 更正: 把由spDic做初始,改成由sceneFrom中cansetFrom的outSPDic来初始 `T`; |
| TODO4 | 改下把中间帧超时反馈失败的 (及所有传染到的),在actYes超时未反馈后,计SP- `T`; |
| TODO5 | 改下把中间帧反馈成功的 (或被唤醒的),在feedbackTOR反馈匹配后,计SP+ `T`; |
|  | 另外: 其中唤醒的,应先把传染时的负1回滚一下,再把正1加上 `T`; |
| TODO6 | 改下把末帧超时未反馈负价值的 (及所有已达到末帧的canset),在actYes超时未反馈后,计SP+ `T`; |
| TODO7 | 改下把末帧反馈负价值的 (及所有已达到末帧的canset),在feedbackTOP反馈匹配后,计SP- `T`; |
| TODO8 | 把canset竞争由InSP改成由OutSP来计算 `T`; |
| 回测 | 继续上表末的试错训练: `试跑几次,饿,上扔无皮果`,看下canset的竞争情况会不会快速响应竞争变化; |
| 结果 | 经测跑三四次`饿,无皮果,飞,吃`后,是可以快速响应变化,使这些有用的canset快速具备竞争力 `存为FZ955x6`; |

**小结: 上表支持了OutSPDic,然后可以快速响应试错效率提升了,但是这改动会影响OutSP评分,所以TO阶段的训练也需要重新训练下: 下面重跑下训练FZ66,以及继续推进31183的训练项3-无皮果动机;**

***

## n32p01b 回测
`CreateTime 2024.06.21`

上节支持了OutSPDic,本节回测;

| 32013 | 在FZ955基础上接着继续31183-训练项3的`无皮果动机`及后续训练项 (参考31183) |
| --- | --- |
| 训练项1 | 测下newRCanst,absRCanset,newHCanset,absHCanset四个都能执行到 `在31183完成过,此处重训练下`; |
|  | > 步骤4: (`路边出生,饿,路上扔带皮果,扔棒去皮,手动飞至,触发吃掉`) x 路下n次 (注:鸟与果相对位置要稳定些) |
|  | > 注意: 跑前几次还好,后几次最好边观察hSolution有激活结果,边慢训几次,以使absHCanset可执行到; |
|  | > 说明: 在FZ913基础上,按此步骤4重训练,得到FZ964 `T`; |
| 训练项2 | 试错训练: 测下可以快速响应Canset的OutSPDic变化,使有用的canset快速具备竞争力; |
|  | > 步骤5: 跑三四次`饿,无皮果,飞,吃`,存为FZ965 `T`; |
| 训练项3 | 测下无皮果动机; |
|  | > 训练6 `FZ965,路下出生,点击饿` (说明: 观察日志,看能否得到无皮果动机); |
|  | > 日志: HDemand`A4584(向89,距12,果)`,`A4467(向92,距12,果)`,`A4075(向86,距12,果)`等,可见无皮果动机ok `T`; |
| 训练项4 | 学去皮: 学会去皮(压去皮) `T`; |
|  | > 训练步骤: 1.在去皮动机生成H无皮果后 2.扔有皮果 3.扔棒去皮 4.feedbackTOR反馈到无皮果 5.生成扔棒去皮H经验 |
|  | > 具体步骤: FZ965,路上出生,点击饿,在生成H无皮果后,扔有皮果,扔木棒去皮,上飞吃掉; |
|  | > 取一些训练中关键日志如下: |
|  | H无皮果日志1: `flt1 A4204(距12,果)` |
|  | H无皮果日志2: `flt1 A4592(向88,距0,果)` |
|  | H无皮果有反馈成立1: `flt2 R feedbackTOR反馈成立:A4584(向89,距12,果) 匹配:1 baseCansetFrom:F4622[↑饿-16,4果,飞↑,4果,吃] 状态:CS_Besting` |
|  | H无皮果有反馈成立2: `flt2 R feedbackTOR反馈成立:A4204(距12,果) 匹配:1 baseCansetFrom:F4565[↑饿-16,3果,飞↑,3果,吃] 状态:CS_Besting` |
|  | 学会去皮HCanset经验1: `flt3 Canset演化> AbsHCanset:F5611[M1{↑饿-16},A4204(距12,果)] toScene:F4565[↑饿-16,3果,飞↑,3果,吃] 在2_1帧:A4467(向92,距12,果)` |
|  | 学会去皮HCanset经验1: `flt3 Canset演化> NewHCanset:F5612[M1{↑饿-16}] toScene:F4622[↑饿-16,4果,飞↑,4果,吃] 在1帧:A4584(向89,距12,果)` |
|  | > 说明: 如上日志可见,此训练过程能够生成扔棒去皮H经验,并存为:FZ966 `T`; |
| 训练项5 | 用去皮: 能生成H有皮果动机; |
|  | 其间问题1: 其间发现反思不通过,加训[饿,更饿]和[饿,皮果,去皮,食之] (参考32016-方案); |
|  | 其间解决1: 具体解决的训练步骤:`饿,更饿,更饿,皮果,扔棒去皮,飞至食之` (这个可以加强饿更饿,并且减弱饿皮果更饿),存为FZ967 `T`; |
|  | 结果日志1: `> F5841行为化前 的 子任务分:-9.91 > 当前任务分(饿):-10.86 =====> 已通过`,日志可见,它可以反思通过了; |
|  | 继续回测1: 继续测有皮果动机步骤: `FZ967,饿`,效果如下: |
|  | > H有皮果动机ok: `TCDemand.m  39] A4006(向87,距12,皮果)`,日志可见,可以生成H有皮果动机了 `T`; |
|  | 其间问题2: 测得在`更饿`发生后任务会失效,导致来不及生成"有皮果动机",这个root就不再有激活机会了 |
|  | 其间解决2: 在n32p04中,已经调整"持续饿感"的"任务失效机制":调整为负mv反馈后任务不失效,此问题已修复 (参考n32p04); |
|  | 继续回测2: 继续测有皮果动机步骤: `FZ967,饿`,效果如下: |
|  | > H有皮果动机执行行为化ok: `R行为化中间帧下标 (1/10) A3955(向90,距13,皮果)` |
|  | 结果: 随着以上两个BUG的修复,H有皮果动机已经彻底ok,下面开始训练学搬运 `转训练项6`; |
| 训练项6 | 学搬运: 学会搬运(踢坚果); |
|  | 说明: 前几个训练项已经在下面直至n32p04修了许多个BUG,此处训练项6的推进转到n32p05中继续干 (转n32p05); |
| 训练项7 | 用搬运: 使用搬运; |

**小结: 上表就是本节的训练测试的主表,下面都是些中途发现BUG并修复的记录;**

| 32014 | BUG-生成canset时的indexDic又有越界问题了,查下原因 |
| --- | --- |
| 原因 | 在NewHCanset时,使用了self.realCansetToIndexDic映射,但这个映射此时也还在更新中,导致有更新后,越界了; |
| 修复 | 创建NewHCanst时先把realCansetToIndexDic.copy(),这样后续再更新,就不会同步追加到已经创建的NewHCanset映射中 `T`; |

```txt
32015: 测有皮果动机-测得反思因噎废食不通过问题
复现: `FZ966,路下出生,点击饿`
说明: 如下日志:`1.hSolution输出了有皮果的解 2.行为化前先反思并得到P20条解 3.形成了子任务 4.反思没通过`
反思: 的双方为: A任务本身是为了防止更饿 <= PK => B以往失败时它也会更饿;
    > 分析: 以往的更饿,与执行Canset无关,即使不执行,它也会变更饿;
示例1: 喝药没治好病,只能说明药没用,而不是药使你继续病 (用药的后续病重程度 ≈ 不用药的持续病重程度);
示例2: 中药用错即为毒,确实会让病更重 (用药的后续病重程度 > 不用药的持续病重程度);
示例3: 喝药非常苦,不管病好不好,药苦是药带来的;
    > 分析: 下方日志中,其实二者是约等于的,此时不应因此而`因噎废食`;
方案1: 留下50%的余地,即`A x 1.5 < B`时才判定为反思不通过 (参考3005a-方案1 已经做过);
方案2: 不应该取`最严重子任务分`,因为那个最严重也未必就会发生,所以取每个子任务的平均分 `先选定此方案`;
方案3: 不应该取`最严重子任务分`,因为那个最严重也未必就会发生,所以按每个子任务的pFo的强度来分配评分比重求出最终综合得分 `现在本就支持sp率影响评分`;
抉择: 方案2简单便捷,并且本来就支持spScore对评分的影响,暂选定,进行实践 `已实践 T`;
回测: 经回测,改完方案2后还是反思不通过,平均分依然很高,导致不通过 `转32016 T`;
3438 [11:14:52:084 TO        TCSolution.m  39] =============================== 12 hSolution ===============================
3439 [11:14:52:084 TO        TCSolution.m  39] flt2 目标:A4628(向89,距11,果) 已有S数:0
3440 [11:14:52:160 TO    TCSolutionUtil.m 117] 第2步 H转为候选集:29 - 中间帧被初始传染:9 = 有效数:20
3441 [11:14:52:161 TO    TCSolutionUtil.m 198] 第5步 HAnaylst匹配成功:29
3442 [11:14:52:161 TO    TCSolutionUtil.m 206] 第6步 H排除Status无效的:29
3443 [11:14:52:161 TO    TCSolutionUtil.m 212] 第7步 H排除Infected传染掉的:20
3444 [11:14:52:162 TO    TCSolutionUtil.m 218] 第8步 H排除FRSTime来不及的:18
3445 [11:14:52:164 TO            AIRank.m 189] flt3 H0. I<F3998 F4228[↑饿-16,4果皮]> {0 = 0;}  (null):(分:0.00) [CUT:0=>TAR:1]
3446 [11:14:52:165 TO            AIRank.m 189] flt3 H1. I<F3998 F4241[↑饿-16,4果皮]> {0 = 0;}  (null):(分:0.00) [CUT:0=>TAR:1]
3447 [11:14:52:166 TO            AIRank.m 189] flt3 H2. I<F3998 F4327[↑饿-16,4果皮,4棒]> {0 = 0;}  (null):(分:0.00) [CUT:0=>TAR:1]
3448 [11:14:52:167 TO            AIRank.m 189] flt3 H3. I<F3998 F4249[↑饿-16,4果皮]> {0 = 0;}  (null):(分:0.00) [CUT:0=>TAR:1]
3449 [11:14:52:168 TO            AIRank.m 189] flt3 H4. I<F3998 F4342[↑饿-16,4果皮,4棒]> {0 = 0;}  (null):(分:0.00) [CUT:0=>TAR:1]
3450 [11:14:52:169 TO      TCRefrection.m  39]
3451 [11:14:52:169 TO      TCRefrection.m  39]
3452 [11:14:52:169 TO      TCRefrection.m  39] =============================== 12 TCRefrection反思 ===============================
3453 [11:14:52:169 TO      TCRefrection.m  39] F4228[↑饿-16,4果皮] CUT:0 无效率:0.00
3454 [11:14:52:169 TO      TCRefrection.m  67] 反思评价结果:已通过 (解决任务奖励分10.0 Canset风险:0.00 懒分:0.0 = 10.0)
3455 [11:14:52:169 TO    TCSolutionUtil.m 256] 第9步 H求解最佳结果:F4228 {}
3456 [11:14:52:185 TO AIThinkingControl.m 271] energy > delta:-1.00 = energy:18.00
3457 [11:14:52:190 TO        TCSolution.m 278] > newH 第29例: eff: sp:{} I scene:F3998 canset:F4228 (cutIndex:0=>targetIndex:1)
3458 [11:14:52:199 TO     TCRecognition.m  35]
3459 [11:14:52:199 TO     TCRecognition.m  35]
3460 [11:14:52:199 TO     TCRecognition.m  35] =============================== 12 行为化前 反思识别 ===============================
3461 [11:14:52:199 TO     TCRecognition.m  35] regroupFo:F6332[M1{↑饿-16},M1{↑饿-16},M1{↑饿-16},M1{↑饿-16},A4227(向91,距13,皮果)]
3482 [11:14:53:443 TO          AIFilter.m 351] 过滤器: 总91需20 主:0.22 => 剩:20
3483 [11:14:53:443 TO           TIUtils.m 470]
3484 [11:14:53:443 TO           TIUtils.m 470] 时序识别结果 P(20条) R(0条)
3485 [11:14:53:633 TO          AIFilter.m 191]
3486 [11:14:53:633 TO          AIFilter.m 191] 时序二次过滤后条数: 剩3 >>>>>>>>>>>>>>>>>>>>>
3487 [11:14:53:634 TO          AIFilter.m 192] 	1. F1351[M1{↑饿-16},A1348(距47,向147,皮果)]
3488 [11:14:53:635 TO          AIFilter.m 192] 	2. F518[M1{↑饿-16},A515(距32,向72,皮果)]
3489 [11:14:53:636 TO          AIFilter.m 192] 	3. F1323[M1{↑饿-16},A1320(距63,向58,皮果)]
3490 [11:14:53:637 TO           TCDebug.m  61] [TCRecognition.m => TCDemand.m] 操作计数:811 用时:****** (616) (读:0 写:0)
3491 [11:14:53:639 TO          TCDemand.m  39]
3492 [11:14:53:639 TO          TCDemand.m  39]
3493 [11:14:53:639 TO          TCDemand.m  39] =============================== 13 subDemand ===============================
3494 [11:14:53:639 TO          TCDemand.m  39] 子任务数:1 baseFo:F6331[M1{↑饿-16},A4227(向91,距13,皮果)]
3495 [11:14:53:640 TO          TCDemand.m  76] 	 pFo:F1351[M1{↑饿-16},A1348(距47,向147,皮果)]->{饿-16.00}
3496 [11:14:53:641 TO          TCDemand.m  76] 	 pFo:F1323[M1{↑饿-16},A1320(距63,向58,皮果)]->{饿-16.00}
3497 [11:14:53:642 TO          TCDemand.m  76] 	 pFo:F518[M1{↑饿-16},A515(距32,向72,皮果)]->{饿-12.80}
3498 [11:14:53:642 TO      TCRefrection.m  39]
3499 [11:14:53:642 TO      TCRefrection.m  39]
3500 [11:14:53:642 TO      TCRefrection.m  39] =============================== 13 行为化前 反思评价 ===============================
3501 [11:14:53:643 TO      TCRefrection.m 104] > 最严重子任务分(饿):-10.45 > 当前任务分(饿):-10.04 =====> 未通过
3502 [11:14:53:643 TO AIThinkingControl.m 242] TO上轮:失败 等待:0.0 下轮:10 消息:action反思不通过
3626 [11:14:54:645 TO     DemandManager.m 300] 	第1条 饿 评分-10.17 激活成功 	{proto:F6334 pFos:(F3871,F3825,F3566,F3717,F3579,F3597,F3521,F3531,F3543,F3553,F3611,F3629,F3651,F3699,F3644,F3559,F3546,F3534,F3524)}
3627 [11:14:54:646 TO     DemandManager.m 303] Demand竞争 <<<== SUCCESS 共6条
3628 [11:14:54:647 TO            TCPlan.m  51] 取得最终胜利的末枝 >> 取分: K:0x600003f04750_ => V:(null)分
```

```txt
32016-反思子任务迫切度太高,导致反思仍不通过;
//=============================== 13 subDemand ===============================
//子任务数:1 baseFo:F6642[M1{↑饿-16},A4227(向91,距13,皮果)]
//pFo:F1351[M1{↑饿-16},A1348(距47,向147,皮果)]->{饿-16.00}
//pFo:F1323[M1{↑饿-16},A1320(距63,向58,皮果)]->{饿-16.00}
//pFo:F518[M1{↑饿-16},A515(距32,向72,皮果)]->{饿-12.80}
//=============================== 13 行为化前 反思评价 ===============================
//> 行为化前:F4228 的 子任务分:-10.45 > 当前任务分(饿):-10.04 =====> 未通过

分析1. 在经验中,场景pFo饿了不一定会更饿,所以任务分在10.x左右 `即[饿]->{更饿} 只有65%稳定性`;
分析2. 而在反思预测中,饿了看到皮果,的场景pFo,却更大几率会更饿,所以它的评分在15左右 `即[饿,皮果]->{更饿} 却有95%稳定性`;
思路: 可以把[饿]->{更饿}训练的更稳定,或者把[饿,皮果]->{更饿}训练的更不稳定,即以下两个方案都执行下来试下;
方案1: 降低子任务分: 看下多训练几下: 看到有皮果后,去皮并吃到,看能不能把这个几率降低一下;
  > 实践: 训练步骤`饥饿,皮果,扔棒去皮,飞至食之` => 使之知道皮果去皮一吃是可行不再饿;
方案2: 提升父任务分: 当饥饿多次无解,它自然会任务分更高,直到饿的程度足够它冒险尝试;
  > 实践: 训练步骤`饥饿,等一会让它多饿几次` => 使之知道稳定不解决一直会饿;
结果: `转32013-训练项5-其间问题1` 加训并存为FZ967 (最终训练步骤也转32013-其间问题1);
```

```txt
32017: 查为什么H有皮果有动机,但没持续推进求解,日志如下:
问题说明: 在`32013-训练项5`,训练有皮果动机时,发现生成了`H有皮果`,但没执行`hSolution有皮果`,经测,发现更饿发生后,`H有皮果`所在的Root再也没激活了,导致没能持续性去推进它,本节重点解决Root的持续性问题;
回顾: 以前其实已经搞过Root的持续性 (在TCDemandManager.refreshCmvCacheSort()中,根据其Canset的进度来评分) (参考31052);
//第1条 饿 评分-10.32 激活成功     {proto:F7597 pFos:(F3871,F3825,F3900,F3566,F3717,F3579,F3597,F3521,F3531,F3543,F3553,F3611,F3629,F3651,F3699,F3644,F3559,F3546,F3534,F3524)}
//第2条 饿 评分0.00             {proto:F7592 pFos:(F7304,F7061,F3871,F3825,F3900,F5594,F7175,F3566,F3717,F3579,F3597,F3521,F3531,F3543,F3553,F3611,F3629,F3651,F3699,F3908)}
日志说明: 如上日志,可见,第1条是更饿,第2条并不是没持续性,而是它评分为0,导致它没激活;
1. 查下此处,前一个饿任务明明还在推进"H有皮果",为什么会被评分0分 (所有pFo都无效);
 分析1. 先查下此处挂0的原因: 为什么都无效了,如果有任务在推进中,但是个持续性任务怎么办?新root能快速复用到旧root的成果吗?
2. 然后看下,以前写的任务推进的持续性,能不能有效,帮助他持续推进下去...
 分析2. 以前为保证任务持续性,对root的竞争显然无效,应该在root推进了几层树时,相应的就加几层权,使之可持续;
修复: 在refreshCmvCacheSort()中改为调用score4Demand_Out()后,它不再是0分了,日志:`root(0/2):F7821[M1{↑饿-16}] 评分:10.86`;
回测: 它确实不再是0分了,但仍然没激活,因为它已经是isExpired状态,导致更饿发生后,就不会再激活了,它的一切工作记忆的推进进度也都白白浪费了 `此问题转n32p04`;
```

```txt
3201x-接31184结果: 无皮果动机和有皮果动机,都需要加训和试错;
//1. 加训无皮果RCanset: 路下出生,扔路上无皮果,上飞至,吃掉;
//      > 目标: 使之能稳定的激活上面习得的rCanset[无皮果,飞至,吃掉],然后生成无皮果hDemand;
//2. 加训有皮果HCanset: 生成无皮果HDemand后,扔有皮果,扔木棒去皮,h无皮果反馈成立,生成无皮果hCanset;
//      > 目标: 使之能稳定的激活上面的hCanset[有皮果,木棒,无皮果],然后生成有皮果的hDemand;
//3. 试错训练: 然后试下,这么多各种canset,是否应该多淌一淌,把一条可行的路慢慢淌出来;

明日: 别着急,一步步来,先看下训练项3,无皮果动机是哪来的,是不是rCanset[无皮果,飞至,吃];
结果: 这个表后来没怎么用,因为后来用(无皮果父任务,有皮果子任务,有皮果搬运子子任务 => 然后先搬运,再压破皮,再飞过去吃),这套流程来一节节推进的 (参考下面几节的手稿记录);
```

**小结: 上面针对31183中,训练不顺的那些训练项,制定了更为细节的训练规划;**

TODO测试项3: 测试RCanset有皮果反馈后,能不能继续推后进下一帧;

***

## n32p02 多层子H任务嵌套时的H迁移
`CreateTime 2024.06.09`

起因: 在上节最后的测试中,发现多层H嵌套时,H的迁移应该只支持一层 (测得不支持多层H迁移问题);
1. 原先写的是H的scene必然是rCanset
2. 但其实H的scene可能是baseH,不一定就是rCanset;

示例: H任务在工作记忆中是会多层嵌套的,比如:
1. 无皮果的HCanset为[有皮果,压,无皮果]
2. 然后它的有皮果又有子H任务[路边有皮果,踢,路中有皮果]

问题: 以前H的迁移比R多一层,在这里来看,这是不对的,H不止是一层,它可能多出多层 (因为子H可能嵌套多层);
1. 本节主要分析并支持这种多层下H迁移的情况;

| 32021 | 回顾现有代码之: 多层hCanset合并为单层rCanset; |
| --- | --- |
| 示图 | ![](assets/720_多层HCanset生成为单层RCanset.png) |
| 说明 | 如图,H确实会嵌套,但它的成果会被打包成一个(合并)整体的rCanset; |
| 总结 | 多层H的成果确实是一个rCanset,但成果之前,它确实是多个h嵌套 (即`本表不妨碍本节的问题依然存在`); |

| 32022 | 回顾现有代码之: h迁移也是基于rScene树的; |
| --- | --- |
| 说明 | 现有代码中,H的迁移也是基于RScene树来进行的,所以H也许不会有多层嵌套的问题; |
| 解释 | 即无论H有多少层嵌套,它在长时记忆中都是场景树,比R多一层而已; |
| 结果 | 本表可见,本节多层子H嵌套的问题压根不存在; |

**小结: 从32022的结果可见,本节的问题应该是不存在的,中止之;**

***

## n32p03 整理下：HE模型流程图
`CreateTime 2024.06.15`

| 32031 | HE流程图 |
| --- | --- |
| 彩图 | ![](assets/722_HE模型流程图.png) |
| 说明 | https://www.bilibili.com/video/BV1Xw4m1e7RH/ |
| 注1 | 反馈由外循环推动(现实与智能体循环),意向性推动了内循环(认知与决策循环); |
| 注2 | 识别与求解相对,识别是IN阶段的展开,求解是OUT阶段的展开; |
| 注3 | 此图临时起意,细节上用的名词等并不严谨,但与HE代码的流程是一致的,不影响"表达和理解"其意; |

**小结: 最近试用了一下B站直播功能,分享了HE流程图,顺便把以上模型图画了下;**

***

## n32p04 调整"持续饿感"的"任务失效机制" : 调整为负mv反馈后任务不失效
`CreateTime 2024.07.03`

说明: 在`32017-回测`中,测得连续饿感在任务失效后,也没法激活,但其实它是连续饿感,并且还没解决,应该能`继续推进任务的解决`才对,本节针对此问题做改动与支持;
示例: 当饥饿任务还在解决中,就发生了更饿时,此时原饥饿任务应该继续解决,而不是直接计为isExpired状态就白白浪费掉;

| 32041 | 分析方案 |
| --- | --- |
| 分析 | 饥饿是持续性任务,疼痛不是,比如婴儿已经饿过了,还在找吃的,要不要标一下持续性 (把持续饥饿搞的更深入一些,而不止是隔时间就触发下)? |
| 思路 | 持续饿感,在更饿发生后,说明它还没解决,正应该继续推进解决,而不是失效中断之; |
| 方案 | 根据mv类型,转为是否为持续mv,如果是持续的,那么只要未解决,就永远不会失效(isExpired); |
| TODO1 | 写个方法,用于判断mv为持续性还是单发 (如:饿感为持续性,痛感为单发) `T`; |
| TODO2 | pFo发生负价值反馈时: 单发价值=>已不可挽回计为失效 & 持续价值感=>发生后还会再发生不计失效 `T`; |
| TODO3 | 持续价值的pFo在到期后,发现已经有mv反馈(但它发生后还会发生),所以不可计为失效状态 (反之,一直没反馈,说明已经自然完成,可以计为失效) `T`; |
| 回测 | 第1条 饿 评分-10.86 激活成功 	{proto:F7926 pFos:(F7304,F7061,F3871,F3825,F3900,F5594,F7175,F3566,F3717,F3579,F3597,F3521,F3531,F3543,F3553,F3611,F3629,F3651,F3699,F3908)} |
| 结果 | 根据以上回测日志可见,原饥饿root可以顺利激活了 `T`; |

**小结: 上表为本节主体,改后回测已ok**

```txt
32042-测得`有皮果动机行为化前的反思`又不通过的问题 (参考32016)
说明: 如下,测得新的问题,虽然激活了,但它反思不通过,
//1. 它已经可以激活有皮果动机了,如下日志;
R0. I<F3825 F3986[↑饿-16,4果皮,4棒,4棒,4果,飞↑,4棒,4果,4棒,吃]> {0 = 0;} {9 = S0P16;5 = S0P19;1 = S1P20;10 = S0P2;6 = S0P16;2 = S0P19;7 = S0P16;3 = S0P19;8 = S0P16;4 = S0P19;} H3N2:(分:0.60) [CUT:0=>TAR:10]

//2. 但反思又失败了,看起来像是: 前期训练学认有皮果时跑了上百轮,有了太稳定会饿的副作用;
=============================== 5 subDemand ===============================
子任务数:1 baseFo:F3986[M1{↑饿-16},A3955(向90,距13,皮果),A3958(距89,向18,棒),A3961(向87,距27,棒),A3962(向90,距13,果),飞↑,A3970(向164,距71,棒),A3971(向90,距5,果),A3976(距73,向166,棒),A3979(吃1)]
	 pFo:F200[M1{↑饿-16},A197(向136,距101,皮果)]->{饿-16.00}
	 pFo:F387[M1{↑饿-16},A384(向23,距67,皮果)]->{饿-16.00}
	 pFo:F1351[M1{↑饿-16},A1348(距47,向147,皮果)]->{饿-16.00}
=============================== 5 行为化前 反思评价 ===============================
> F3986行为化前 的 子任务分:-11.20 > 当前任务分(饿):-10.86 =====> 未通过

方案1: 可以试下32016的解决方法应该还是有效果的;
  > 试了下,这方法倒是管用,就是跑着比较慢,毕竟在当时`30092-步骤1`时,对有皮果跑了200次,这副作用有点大,形成阴影了,看到皮果就认定了会更饿;
  > 所以: 这方法虽然管用,但有点麻烦,暂不考虑了;
方案2: 在反思时,可以把任务求解的进度分考虑入内,毕竟在DemandManager中root竞争时就用了进度总分;
  > 此方案简单可行也合理 `95% 选定`;
TODO1: 把原来refreshCmvCacheSort()中求含进度的任务总分,封装成一个方法(封装成了progressScore4Demand_Out)来计算含进度的任务分 `T`;
TODO2: 在actionRefrection行为化反思中,把任务分改为调用"含进度的任务总分",这样应该稳稳的反思能通过了 `T`;
回测日志如下:
日志1. > F3986行为化前 的 子任务分:-11.20 > 当前任务分(饿):-16.49 =====> 已通过
日志2. =============================== 5 行为化Fo ===============================
日志3. R行为化中间帧下标 (1/10) A3955(向90,距13,皮果)
回测说明: 经回测,方案2实践后,有皮果动机反思可以顺利通过了,主要是含了进度影响后,任务总分更迫切了 `T`;
```

**小结: 上表是个行为化前反思不通过的BUG,调整任务为加上进度分影响后,ok了;**

***

## n32p05 学搬运
`CreateTime 2024.07.05`

本节接续`32013的训练项6学搬运`,把训练中遇到的问题或细节等在本节处理修复等;

| 32051 | 训练: 学搬运 (参考32013-训练项6) |
| --- | --- |
| 训练项6 | 学搬运: 学会搬运 (学会踢坚果能使坚果位置变化); |
| 步骤 | `FZ967,饿,等看到有皮果动机后,扔路边带皮果,踢到路上` x 5次; |
| 重点日志1 | `flt1 A4006(向87,距12,皮果)` (有皮果动机); |
| 重点日志2 | `flt2 Canset演化> NewHCanset:F7772[M1{↑饿-16},A7764(向190,距0,皮果),踢↑,A7771(向94,距9,皮果)] toScene:F3986[↑饿-16,4果皮,4棒,4棒,4果,飞↑,4棒,4果,4棒,吃] 在1帧:A3955(向90,距13,皮果)` (学会搬运); |
| 注 | 步骤执行中,如果点击饿后,半天没皮果动机,那多等一会应该就有了 (它需要canset池自行做试错); |
| 结果 | 将以上训练跑完后 `T 存为FZ977`; |

***

## n32p06 用搬运
`CreateTime 2024.07.06`

本节接续`32013的训练项7用搬运`,把训练中遇到的问题或细节等在本节处理修复等;

| 32061 | 训练: 用搬运 (参考32013-训练项7) |
| --- | --- |
| 训练项7 | 用搬运: 使用搬运 (主动有目的的踢坚果); |
| 步骤 | `FZ977,饿,等看到距0有皮果动机后,扔距0带皮果` -> 然后看它自己把坚果踢到路上; |

***

## n32pxx TODO备忘
`CreateTime 2024.07.07`

1. 2024.07.07: 以瞬时记忆做初始唤醒 (这个可能不需要支持,因为以前的前段已经有瞬时的支持了,那么这个前段必然会出现在工作记忆中,所以它也是可以做为初始唤醒作用的,而工作记忆中的初始唤醒现在就是支持的);

<br><br><br><br><br>
