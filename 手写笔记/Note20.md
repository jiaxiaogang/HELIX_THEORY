# v2.0版本三测
　　本章对v2.0进行第三轮测试训练，此前一测为2019年5月，二测为2019年12月，本轮改进主要是在上轮二测后，改进了以下几个方面:
  1. 反向反馈类比，更快更全面的抽象。
  2. 以及决策期对网络更全面的使用（主要是SP方面）。
  3. 对短时记忆更全面的支持。
  4. 以及对外循环更好的支持完善。

　　所以本次三测，或有望成为发布v2.0前的最后一轮测试，且在上轮二测中已经基本测通了认知期的代码，以及测了大多数决策期的代码。

***

<!-- TOC -->

- [v2.0版本三测](#v20版本三测)
  - [n20p1 三测: 规划训练1](#n20p1-三测-规划训练1)
  - [n20p2 三测: 规划训练2](#n20p2-三测-规划训练2)
  - [n20p3 三测: 耗时BUG](#n20p3-三测-耗时bug)
  - [n20p4 从`ft稀疏码`到`MultiMatch`](#n20p4-从ft稀疏码到multimatch)
  - [n20p5 识别算法迭代](#n20p5-识别算法迭代)
  - [n20p6 PM理性评价](#n20p6-pm理性评价)
  - [n20p7 三测: 规划训练3](#n20p7-三测-规划训练3)
  - [n20p8 决策流程控制整理->支持HNGL时序、支持Fo](#n20p8-决策流程控制整理-支持hngl时序支持fo)
  - [n20p9 继续三测](#n20p9-继续三测)
  - [n20p10 PM理性反思评价准确度迭代](#n20p10-pm理性反思评价准确度迭代)
  - [n20p11 TIR_Fo识别率迭代](#n20p11-tir_fo识别率迭代)
  - [n20p12 减少依赖反向价值](#n20p12-减少依赖反向价值)
  - [n20p13 用时序识别V2_继续三测](#n20p13-用时序识别v2_继续三测)
  - [n20p14 用时序识别V1.5_继续三测](#n20p14-用时序识别v15_继续三测)
  - [n20p15 在20143上_继续三测](#n20p15-在20143上_继续三测)
  - [n20p16 评价器](#n20p16-评价器)
  - [n20p17 训练飞行方向](#n20p17-训练飞行方向)
  - [n20p18 PM评价BUG-反向反馈类比迭代为反省类比](#n20p18-pm评价bug-反向反馈类比迭代为反省类比)
  - [n20p19 反省类比代码规划](#n20p19-反省类比代码规划)
  - [n20p20 反省类比代码实践](#n20p20-反省类比代码实践)
  - [TODOLIST](#todolist)

<!-- /TOC -->

***

### n20p1 三测: 规划训练1
`CreateTime 2020.06.06`

| 20011 | 训练步骤细节分析版: | STATUS |
| --- | --- | --- |
| A | 果可吃 | T |
|  | **1. 直投,飞右上,直投** | T |
| B | 远果不能吃 | T |
|  | **1. 重启,远投右,饿** | T |
|  | **2. 重进成长页,远投右** | T |
| C | 学飞 |  |
|  | **1. 重启,远投右,飞右** |  |
| D | 用飞 |  |
|  | **重启,远投上,马上饿** |  |

**20012完成标记**

* A组: 果可吃
  - 直投,飞右上,直投
    ```c
    //类比出无经纬果可吃;
    //1. 正向类比-抽象时序
    STEPKEY------------------------ 正向反馈类比 START ------------------------
    F8[A4(),A5(速0,宽5,高5,形2.5,距0,向→,红0,绿255,蓝0,皮0,经199,纬375),A1(吃1),A6()]->M3{64}
    F3[A1(速0,宽5,高5,形2.5,经207,纬368,距0,向→,红0,绿255,蓝0,皮0),A1(吃1),A2()]->M1{64}
    STEPKEY--->> 构建时序:F11[A10(速0,宽5,高5,形2.5,距0,向→,红0,绿255,蓝0,皮0),A1(吃1)]->M7{64}
    //2. 正向类比-抽象概念
    STEPKEY—> 构建概念:A10(速0,宽5,高5,形2.5,距0,向→,红0,绿255,蓝0,皮0)
    STEPKEY具象1:A5(速0,宽5,高5,形2.5,距0,向→,红0,绿255,蓝0,皮0,经199,纬375)
    STEPKEY具象2:A1(速0,宽5,高5,形2.5,经207,纬368,距0,向→,红0,绿255,蓝0,皮0)
    ```
* B组: 远果不能吃
  - 远投右
    ```c
    //B1-远投 (预测吃可饱);
    STEPKEY时序识别: SUCCESS >>> matchValue:0.500000 F11[A10(速0,宽5,高5,形2.5,距0,向→,红0,绿255,蓝0,皮0),A1(吃1)]->M7{64}
    //B1-饿 (真实为饿)
    STEPKEY------------------------ 反向反馈类比 START ------------------------
    STEPKEY----> 反向反馈类比 CreateFo内容:F2[A1(距0),A1(吃1)]->M10{64}
    STEPKEY----> 反向反馈类比 CreateFo内容:F2[A1(距46,经202,纬508)]->M13{-51}
    //B1-进R+,输出吃,但并不能解决问题;
    -> SP_Hav_isOut为TRUE: A1(吃1)

    //B2-远投右 (预测更饿)
    STEPKEY时序识别: SUCCESS >>> matchValue:1.000000 F13[A7(速0,宽5,高5,形2.5,向→,红0,绿255,蓝0,皮0,距54,经191,纬531)]->M8{-51}
    //B2-进R- (进入SP行为化,得到了GLDic,但失败了,因为此时的行为应该是飞近,但没学过飞)
    ------SP_GL行为化:距37 -> 距0
    //B2-进R+ (循环前两帧,一帧越界`参考bug7`,一帧输出吃`同B1`)
    <警告> 行为化概念无效
    -> SP_Hav_isOut为TRUE: A1(吃1)
    ```
* C组: 学飞
  - 远投右,飞右
    ```c
    //C1-飞右 (进入SP-GL行为化)
    ------SP_GL行为化:距52 -> 距0
    //BUG-发现GL找不以索引glAlg,导致行为化失败;
    ```


| 20013_BUG | STATUS |
| --- | --- |
| 1. 活跃度消耗在决策每轮循环调用之前,导致决策循环完未消耗活跃度; | T |
| 2. demand.fo不应期不工作,导致每轮循环联想同一解决方案; | T |
| 3. 训练到C时,远投右,预测时序为mv-,应该进TOP.R-,但却进了P+; | T |
| 4. 训练到B1点击饿时,进了R+,而不是R- (预测为正,实际为负); | T |
| > 正常的,因为预测为正,所以会R+输出`吃` (虽然并吃不到); | T |
| 5. 当决策完成时,是否主动观察自身价值状态 (比如输出吃,观察饥饿状态是否变化); | T |
| > 暂不,因为尽量让imv自然发生,像吃时不会马上饱,而是有味觉吸引着继续; | T |
| 6. TOR.R-中indexOfAbsItem方法得到-1失败的bug | T |
| 7. 有outModel.actionIndex>fo.content.count的问题,导致"行为化概念无效"; | T |
| > 因为B组共三帧,最近一帧右投预测不能吃,R-行为化失败,因为不会飞; | T |
| > 倒数二帧,是B1的马上饿,输出吃,预测可饱,R+行为化失败,因为`吃`已输出,所以越界; | T |
| 8. C1训练右飞两次后,为何还是找不到距离变小索引; |  |
| > 分析原因: 因为在用pAlg索引找,未经历过飞至距0,所以无法找到; |  |
| > 修复方案: 改为由sAlg索引,找s出发,变小接近p; |  |
| > 经查,修改为sAlg依然没有glAlg,所以查内类比是否根本没构建`距变小`节点; |  |
| 9. C1飞右后一帧,被识别为距0果,导致前后内类比成了`距71->0`,这显然不对; |  |
| > 经查因为飞后一帧的速度!=0,所以无法全含,只能纯相似匹配,导致识别为距0果; |  |
| > 修复方案1: 飞到0,吃掉坚果->{mv+} |  |
| > 修复方案2: 训练不同方向飞 (速度也不同),从而抽象出无速度坚果; |  |
| > 执行: 将C1改为向各种方向飞,先抽象出无速坚果,再专注训练飞变近; |  |


<br/><br/><br/><br/><br/>


### n20p2 三测: 规划训练2
`CreateTime 2020.06.18`

> 上节中,因BUG8和9导致速>0的问题,本节调整训练方式,尝试找出更少步骤的新版训练步骤;

* **20021 >> A组-构建无速果,远果不能吃**
  - A1右投
    - `A1(速0,高5,经217,纬536,距56,向→,皮0)` `下简称A1`
  - A2饿
  - A3右飞
    - `A2(高5,经217,纬536,距56,向→,皮0,速-0.000002)`
    - matchA_Seem:`A1`
    - `F4[A1]`
    - matchF:`F2[A1]->M1{-51}`
    - `A3(高5,经217,纬536,向→,皮0,距52,速-9.980625)`
    - `F6[A1,A1]`
    - matchF:`F2`
  - A4右上飞
    - `A4(速0,高5,经217,纬536,向→,皮0,距52)`
    - `A5(高5,经217,纬536,向→,皮0,距50,速-6.417388)`
  - A5饿
    - 正向类比:`F2[A1]->M1{-51}`
    - 正向类比:`F10[A3,A4,A2(飞↗),A5]->M3{-51}`
    - 构建概念:`A12(高5,经217,纬536,向→,皮0)`
    - 构建时序:`F13[A12(高5,经217,纬536,向→,皮0)]->M7{-51}`
* **20022 >> B组-果可吃**
  - B1重启,直投
    - `A6(速0,高5,向→,皮0,经207,纬368,距0)` `下简称A6`
    - matchA_Seem:`A1`
    - `F14[A1]`
    - matchF:`F2[A1]->M1{-51}`
    - 反向类比:`F2[A1]->M1{-51}`
    - 反向类比:`F16[A6(速0,高5,向→,皮0,经207,纬368,距0),A1(吃1),A7()]->M8{64}`
    - 构建S:`F2[A1(经217,纬536,距56)]->M10{-51}`
    - 构建P:`F2[A1(经207,纬368,距0),A1(吃1),A7()]->M13{64}`
  - B2飞右上
    - `A2(飞↗)`
    - `F19[A1,A2(飞↗)]`
  - B3直投
    - `A10(速0,高5,向→,皮0,经199,纬375,距0)`
    - matchA_Seem:`A6`
    - `F20[A1,A2(飞↗),A6]`
    - matchF:`F16[A6,A1(吃1),A7()]->M8{64}`
    - 内类比:`F20[A1(速0,高5,经217,纬536,距56,向→,皮0),A2(飞↗),A6(速0,高5,向→,皮0,经207,纬368,距0)]`
      - 内类比 (大小) 前: 纬536 -> 纬368
      - 内类比 (大小) 前: 距56 -> 距0
      - 内类比 (大小) 前: 经217 -> 经207
    - 正向类比:
      - Con1`F16[A6(速0,高5,向→,皮0,经207,纬368,距0),A1(吃1),A7()]->M8{64}`
      - Con2`F22[A9(),A10(速0,高5,向→,皮0,经199,纬375,距0),A1(吃1),A11()]->M14{64}`
      - Abs:`F25[A24(速0,高5,向→,皮0,距0),A1(吃1)]->M16{64}`
  - B4,重启,右投,饿; `避免让小鸟以为右投就能饱`
  - B5,重启,右投,饿; `避免让小鸟以为右投就能饱`
* **20023 >> C组-训练飞**
  - C1:重启,远投右
    - `A12(速0,高5,纬536,距56,向→,皮0,经211)`
    - matchA_Fuzzy:`A1(速0,高5,经217,纬536,距56,向→,皮0)`
    - `F26[A1(速0,高5,经217,纬536,距56,向→,皮0)]`
    - matchF:`F23[A1,A2(飞↗),A6,A1(吃1)]->M15{64}`
  - C2:飞右
    - `A13(速0,高5,纬536,距56,向→,皮0,经211)`
    - matchA_Fuzzy:`A1(速0,高5,经217,纬536,距56,向→,皮0)`
    - `F27[A1(速0,高5,经217,纬536,距56,向→,皮0),A1(速0,高5,经217,纬536,距56,向→,皮0)]`
    - matchF:`F11[A1,A1,A1,A1]->M4{-51}`
    - 行为化topV2_R+:`-> SP_Hav_isOut为TRUE: A2(飞↗)`
  - C3:重启,右投一飞距离;
  - C4:右飞;

| BUG | STATUS |
| --- | --- |
| 2. 查下C1为何未预测到远果不能吃; |  |
| 3. 在B3时,对F20[]进行内类比,得到距大到距0,变小,查下这里为什么会有距50,因为B组都是直投; |  |
| > `F20[A1(速0,高5,经211,纬518,距50,向→,皮0),A2(飞↗),A6(速0,高5,向→,皮0,经207,纬368,距0)]->F2[A2(飞↗),A1(纬小)]` |  |
| > 查下,F20何来,或者说A1为什么会出现在此处; |  |
| 4. 在C2时,行为化先输出了`飞右上`,后输出了`吃`,查下为什么飞右上后,直接就输出吃了?因为飞右上,并没有直接让坚果的距离为0,此时是吃不到坚果的; |  |
| 5. 输出飞右上行为后,并没有解决Demand,却中止的问题; `转至todo2` | T |
| 6. 在B组之后,需要插入右投马上饿,否则会将C1的远果预测为mv+,查下为什么? |  |
| 7. 在C4飞到距离0时,为什么反而识别成了距7果? |  |

| TODO | STATUS |
| --- | --- |
| 1. 用于描述外观的:`红绿蓝角宽高`,调试时看着眼累,可保留`高`,其它去掉,不影响且直观; | T |
| 2. DemandModel.status完成,只会发生在两种情况下,1是任务被抵消(如饱腹感),2是活跃度为0(如想不到方法了); |  |
| 3. 对飞行8个方向的调试,要支持根据坚果方向,飞向不同方向,以保证飞近; |  |
| 4. 测试_GL行为化,找不到glAlg的BUG,是否ok了; | T |
| > 经测,依然无法联想到glAlg,测试方式为将B组后,插入,远投右,马上饿,重启三步; | T |
| 5. 对决策系统,写打印日志,进行测试; |  |
| 6. 测试任务未完即断的问题 (已修复bug,但还没测); |  |
| 7. 查下fuzzyAlg匹配,是不是模糊条数少会优先,比如(a1,b0),会匹配到(a1,b2),而不是(a1,b1,c1); |  |


<br/><br/><br/><br/><br/>


### n20p3 三测: 耗时BUG
`CreateTime 2020.06.24`

| 20031 | 找不到glAlgBUG; |
| --- | --- |
| 简介 | 无法联想到glAlg的bug,因为`SP构建线`和`GL构建线`本来就不同,所以找不到glAlg; |
| 分析 | 现有代码仅支持从P找一层抽象,而这无法与GL找到交集,故联想失败; |
| 示图 | ![](assets/268_三测找不到glAlg的BUG修改方案.png) ![](assets/269_三测找不到glAlg的BUG可视化测试.png) |
| 图解 | 1. GL和SP的构建并非同时,但从理性考虑,其越相似,GL也同时可对P有效; |
|  | 2. 用A3找A1,优先从纵向`找抽具象有关系`的,其次用`相似度匹配`个有相似的; |
| 改测 | 1. 见图2,发现确实存在抽具象关联,不过GL为具象,P为抽象; |
| 结果 | 先仅对GL和P做mIsC的判断,至于是否要支持相似度以后再说; |



<br/><br/><br/><br/><br/>


### n20p4 从`ft稀疏码`到`MultiMatch`
`CreateTime 2020.06.25`

起因:
1. posX/Y并不是放弃行为化,而是因为本来posX/Y符合ft稀疏码,所以根本不需要行为化;
2. fuzzy匹配充满不确定性,而决策害怕这种不确定性,故习得ft稀疏码,更全面的经验为了更好使用;
3. fuzzy需要计算推测(即相似度模糊匹配),而ft稀疏码不需要计算,这符合`类比结果保留`原则;
4. 类比时,不同不表示无信息,比如8cm苹果和9cm苹果并不表示苹果无大小;

| 20041 | ft稀疏码的两个解决方案 |
| --- | --- |
| **激进方案** | 可以写ft稀疏码索引,并在抽象概念中引用,比如苹果的大小为(6cm-13cm)`5%`; |
| 缺点 | ft稀疏码索引不好维护,比如3比5得到(f3t5),而(f3t5)比8得到(f3t8),但he的知识是不能变动,只能新增的; |
| 优点 | 符合将全面经验存留,并在今后使用的原则; |
| **保守方案** | 不特意写ft,仅在匹配时,对conPorts的同区稀疏码取出,并得到大概范围`95%`; |
| 缺点 | 性能不如方案一好,也犯下了经验不全,实时运算收集的错误; |
| 优点 | 现在这需求并不急,而这些对当前训练的影响,可以先转由别的方法解决 `见20042`; |

| 20042 | MultiMatch |
| --- | --- |
| 比如 | fuzzy带来posX/Y的要求,而这显然并不需要,的问题; |
| 解决 | 多识别:多个全含识别,替代fuzzy匹配,即可更全面预测,又避免了fuzzy带来的不确定性; |
| 举例 | 可将远果,识别为:`果可吃`和`远果不可吃`,两个全含概念与时序; |

| 20043 | fuzzy算法的本质矛盾 |
| --- | --- |
| 1 | 要求,fuzzy算法模糊匹配到`距离`; |
| 2 | 要求,fuzzy不能模糊匹配到`位置`; |
| 问题 | 这二者,是矛盾的,我们不能要求同样的代码逻辑下,对二者的结果要求截然相反; |
| 示图 | ![](assets/270_fuzzy算法矛盾问题分析图解.png) |
| 解读 | 本图,可用来指导迭代时序价值预测算法,即利用稀疏码尽量避免价值预测的不确切性; |

<br/><br/><br/><br/><br/>

### n20p5 识别算法迭代
`CreateTime 2020.06.27`

　　因fuzzy带来的不确定性，原则上与螺旋熵减机的熵减相违背，本节将针对这一点，对识别算法进行迭代。本节共分两个部分进行迭代，一是取消概念识别中的fuzzy模糊部分，二是对时序识别进行迭代使之支持更加确切的时序与价值预测。

| 20051 | MatchAlg部分->代码变动规划 |
| --- | --- |
| 1 | 取消tirAlg算法的fuzzy算法,仅保留全含; |
| 说明 | 因为fuzzy是熵增的,而不是熵减,无论如何都会带来不确定性,蝴蝶效应致其后运转混乱; |

| 20052 | MatchFo部分->代码变动规划 |
| --- | --- |
| 规划 | 对tirAlg的全含结果进行refPorts联想,查看其是否价值稳定; |
| 1 | 如果稳定,可直接返回匹配度最高的时序识别结果 `->识别成功`; |
| 2 | 如果不稳定,即价值时正时负,则进行以下操作: |
|  | a. 根据protoAlg中,找出独特部分的指引 (如距8,pos9等...); |
|  | b. 顺着时序具象,找出稳定的结果,如有距果总是不能吃,则返回结果 `->识别成功`; |
| 注 | 因为protoAlg的独特稀疏码可能多条,所以最终稳定的结果,也可能是多条; |
| 性能 | 关于此规划方案,的性能问题,有以下两种方式: |
|  | 1. 学习期间即构建from-to概念,因本身已具象关联,故不采用; `5%` |
|  | 2. 不构建from-to节点,而是在`独特稀疏码`同区指引下,用全含fo向具象找出答案 `95%`; |
| 废弃 | 中止本节开发,转至n20p6分析从理性角度求解; |


<br/><br/><br/><br/><br/>


### n20p6 PM理性评价
`CreateTime 2020.06.28`

　　本节起因为：`距离`与`位置`的矛盾,分析尝试从类比或理性解决,最终演化为在决策中进行PM理性评价 (PM为ProtoMatch略写),评价失败时,转行为化;

| 20061 | 分析 |
| --- | --- |
| 示图 | ![](assets/271_距离与位置的稳定分析.png) |
| 解读 | 如图,距离是稳定的,而位置是不稳定的; |
| 原则1 | 距离与位置能否吃,是一个理性问题 (无论它们导致的价值如何,理性上是可判断的); |
| 原则2 | `距离`与`位置`矛盾的向性应该是向下的,所以应由决策解决; |
| 方案1 | 决策时,理性判定`距0果能吃`,`距>0果不能吃`,`位置不影响`; |
|  | 优点: 可保证在识别与预测中,广泛的精准的放开想法,到决策期再进行理性的可行性判断; |
|  | 比如: 看到电视上的美食,我也会预测是吃的然后想吃,只是决策分析到需要网购发快递; |
|  | 反例: 如果因为价值不稳定,就想都不想吃,就会错过很多熵减的机会 (因为不稳定是可操作的); |

| 20062 | 代码规划 |
| --- | --- |
| 示图 | ![](assets/272_距离与位置的问题应由哪个阶段解决分析.png) |
| 1 | 在识别阶段,仅作全含识别; |
| 2.1 | 在行为化时,用protoAlg独特稀疏码指引,向具象理性判定其是否需实现; |
| 2.2 | 在行为化_GL中,用RelativeFo_ps判断具体稀疏码是否需行为化; |
| 总结 | 本表,仅决定了,此问题需在决策阶段解决,但对代码的规划还太过粗糙,继20063 |

| 20063 | 代码规划2 |
| --- | --- |
| 代码 | 在TOR.R+中,每一帧protoAlg输入,都做下图中操作; |
| 初步示图 | ![](assets/273_距离与位置的问题在决策TOR.R+阶段解决示图.png) |
| 说明 | 图中,以protoAlg特化码`距`为指引,从absFo.conPorts中,找出价值`正`和`非正`,并得到加工结果`62->0`; |
| 疑点 | Q: fo2和fo3怎样与absFo建立的抽具象关联? |
|  | A1: 如果`fo2=[距5,距0,吃]={mv+}`,即可; |
|  | A2: 或`fo2[距9果,吃]`压根不存在,先用protoAlg找同级坚果们,并fuzzy匹配到距62果,再找refPorts,看是否指向{mv+},未指向,说明不能直接吃; |
|  | 总结: 两种回答,同存而各自工作,比如拿桌上苹果吃用A1,将屏幕上的苹果网购回来用A2; |
| A2示图 | ![](assets/274_距离与位置的问题在决策修正示图.png) |
| 说明 | 此图是A2解答的修正示图,步骤如下: |
|  | 1. 用A2模糊匹配到同层A3(距62); |
|  | 2. 并根据A3.refPorts,发现与F1.conPorts无交集,且得到的fo2发现并不直接指向mv+; |
|  | 3. 根据`分区距`找F1.conPorts,找到F3,发现`距0`指向mv+; |
|  | 4. 得出,需将F2的距62,加工成F3的距0; |
| 注 | **PM理性评价,只需mv同区同向即可评价成功,评价失败时,用GL加工稀疏码;** |
| 进阶 | **分析下,用不用对P.refPort和M.conPorts先取交集,再做相符判断;** |

| TODO | DESC | STATUS |
| --- | --- | --- |
| 1 | 删掉TIRAlg的fuzzy功能; | T |
| 2 | 删掉TIRFo写了一半的`更加确切的时序价值预测`代码; | T |
| 3 | 整理TOAlgModel代码,使理性评价与GL的数据兼容至TOAlgModel和决策流程控制中; | T |
| 4 | 写理性决策PM算法; | T |
| 5 | 将理性决策算法PM集成到TOR与决策流程控制中; | T |


<br/><br/><br/><br/><br/>


### n20p7 三测: 规划训练3
`CreateTime 2020.07.03`

| 20071 | 花样训练记录1 |
| --- | --- |
| 1 | 右投,饿,飞右,飞右上,饿; |
| 2 | 直投,飞右上,直投; |
| 3 | 右投,马上饿; |
| 问题 | 点马上饿,未执行到决策中,查下为什么; |

| 20072 | 花样训练记录2 |
| --- | --- |
| 1 | 直投,飞右上,直投; |
| 2 | 右投,饿,飞右,飞右上,饿; |
| 问题 | 在第2步,`SP_GL行为化:距45 -> 距0`,但未找到glAlg:`getInnerAlg: 根据:距45->距0 找:distance小 GLAlg:`; |

| 20073 | 花样训练记录3 |
| --- | --- |
| 前提 | 在记录2中,有找不到glAlg的bug, |
| 1 | 直投 |
| 2 | 右上飞,直投 |
| 3 | 右投,各种方向飞到坚果上; |
| 4 | 右投,直接点飞到坚果上; |
| 5 | 右投,马上饿; |
| BUG1 | 右投时,识别为全含抽象坚果是ok的,但马上饿后,未执行行为化,查下为什么没执行; |
| 分析 | `远果`只会识别为`无距果`,然后TOP.P+找解决方案时,参数MAlg的具象找`距0果`,才可能被解决方案引用; |
| 修复 | 将topPerceptMode()方法中,MatchAlg先取conPorts再取refPorts; |
| BUG2 | 右投,马上饿时,在_Hav行为化中,M在MModel中为无距果,C为有距果,但匹配不到,行为化失败; |
| 分析 | 经查C为M的具象,此处补上MC代码优先,判断mIsC或cIsM都可以; |
| 修复 | 写上MC,并且当MC匹配时,转给PM算法做理性评价; |


<br/><br/><br/><br/><br/>

### n20p8 决策流程控制整理->支持HNGL时序、支持Fo
`CreateTime 2020.07.07`

1. 以往的决策流程控制不支持TOFoModel，本节给予支持。
2. RelativeFo在迭代及时输出行为到外循环后，最后一位HNGL的剔除，由节给予支持。

| TODO | STATUS |
| --- | --- |
| 1. 决策流程控制支持TOFoModel,分别在Finish,Begin,Failure三个流程控制方法中支持; | T |
| 2. _Hav中对HNGL的跳过,直接Finish,并递归到流程控制的Finish方法; | T |

| 20081 | 行为"飞"死循环BUG (为测通`持续飞行功能`) |
| --- | --- |
| 说明 | 右投偏上,饿,会死循环"飞右"输出,查下原因; |
| 分析 | 1. 此处PM时,其实已经输出上一轮飞行了,那么其实位置应该已经变了; |
|  | 2. 当Fo[飞,近]飞行输出后,当前value.baseAlg.status应该=ActYes,因为飞后的结果还未传回; |
|  | 3. 如果传给PM的话,PM会重新根据alg的pm相关参数,进行重新行为化,也就会陷入再飞,再循环的死循环之中; |
|  | 4. 应该等飞行结果传入,并能够在"外推动中"之中,判断出距离变小 (或者用新的protoAlg,继续进行PM循环再飞); |
| TODO1 | 在飞行行为改为飞行结束时,才再做为input传入 (先不改有影响再说); |
| TODO2 | 在_Hav中参数为HNGL时,只将状态改为Finish但不LoopBack (先不设定独立FinishNotLoopBack状态,而是不调用递归方法); |
| TODO3 | 在下轮输入时,需要判断是否与上轮的HNGL匹配 (不用判断HNGL匹配,直接开始新一轮决策即可,比如再飞,或者重下蛋); |
| 结果 | 将_Hav中,HNGL调用递归去掉即可; |
| 进阶 | 改掉BUG后,可在此基础上,进一步测试持续飞行功能; |

| 20082 | 学习飞别的方向 |
| --- | --- |
| 说明 | 在`20081进阶`完成,BUG1也解决后,鸟学会了持续飞行,但发现下面两个问题; |
| 问题 | 一是鸟会连飞直至超过坚果一点,还不会向左下飞修正,如下图: |
| 示图 | ![](assets/275_持续飞行测试示图.png) |
| 分析 | 在第三飞后,坚果在右下方,而不是(0右),导致识别不成(速0,高5,向→,皮0); |
| 方案 | 1. 在多个方向训练吃坚果,使乌鸦知道与方向无关; |
|  | 2. 只要坚果到了乌鸦手上,就识别为无向(0右),因为此时方向没什么意义; `80%` |
| 结果 | 先采用方案2,至于方案1,以后有的是时间慢慢训练,以后再说喽; |

| 20083 | 飞超过的BUG `纬度影响` |
| --- | --- |
| 说明 | 发现鸟向右飞,距0的时候,还继续飞,即飞超了; |
| 日志 | ![](assets/276_持续飞超过的BUG_纬度影响版.png) |
| 分析 | 距0时,识别为,带纬度的坚果,所以对续度的思考,导致了BUG; |

| BUG | STATUS |
| --- | --- |
| 1. 右投,马上饿,飞两步,就停住的BUG (应该是有效的fo解决方案也被不应期了导致); | T |
| 2. 右投偏下,马上饿,偶尔会在内类比处闪退,报NSArray在遍历时,同时操作了; | T |


<br/><br/><br/><br/><br/>

### n20p9 继续三测
`CreateTime 2020.07.11`

　　在上节的测试中，远投坚果后，小鸟可以在饿的时候持续飞过去，并吃掉坚果。但更多时候在这期间会出现各种各样的BUG，本节针对这些继续测试与修复BUG。

| 20091 | "投饿飞吃"后收尾测试 |
| --- | --- |
| 测试 | ![](assets/277_投饿飞吃后收尾测试1.png)![](assets/278_投饿飞吃后收尾测试2.png)![](assets/279_投饿飞吃后收尾测试3.png) |
| 问题 | 1. 第0步：F2214的解决方案太复杂了，成功率当然低。 |
|  | 2. 第3步：第PM中，为什么会对经度进行加工？ |
|  | 3. 第10步：反思为空时，为何判定为否认了。 |
|  | 4. 第N+1步：如何避免错误时序长期为祸。 |

| 20092 | BUG2_PM为什么会对"经"进行加工 |
| --- | --- |
| 分析 | 在PM中,依`经`进行fuzzy排序后; |
|  | 1. 排在首位`经214`,其引用时序,20条,没有一个有效指向cmv价值变化的; |
|  | 2. 在第二位`经214`,其引用时序,有一个指向了{mv-}的 (需加工); |
|  | 3. 在第三位`经209`,其引用时序,有一个指向了{mv+}的 (无需加工); |
| 方案 | 将取首位前3个,改为从sortAlgs中循环取出3-5个有效价值指向的 (因为无效的没用); |
| 最后 | 原逻辑不变,只要这3-5个中,有一个指向{mv+}(即同向),则无需加工; |
| TODO | `方案`中,找出3-5个有效时序,可能要取几十次fo的io操作,为性能,可考虑将fo指针添加havMv字段,使不必取出时序,即可判断是否指向价值 `先不改,后性能不行再改`; |

| 20093 | PM理性评价不稳定的BUG |
| --- | --- |
| 情景 | 判断当前P独特码是否需要加工时,因其先fuzzy,再A-F-M长路径联想,致M指向不确定; |
| 问题 | 以往做法是找到一条`同向mv`,则不必加工,但发现距43时,甚至找到了距0指向正mv,从而未对距43进行加工,这导致了bug,本表重点针对此问题,分析更好的评价方案; |
| 原则 | 这类分析必须放在TO,不能在TI中实现; |
| 示图 | ![](assets/282_判断P独特码是否需加工示图.png) |
| 说明 | 无论是fuzzy,还是后续的指向价值,都是一种预估,因为不会有一模一样的经历; |
| 方案 | 找到首例同向前,将异向评分也存下来,在正式找到同向时,进行综合评分为结果; |
| 比如 | 找到`-64,-64,+64,综合评分为-64,需加工` 或 `-54,+64,综合评分为+10,无需加工`; |


<br/><br/><br/><br/><br/>

### n20p10 PM理性反思评价准确度迭代
`CreateTime 2020.07.15`

　　在训练中发现，即使采用了综合评价（参考20093），距和纬两个码，几乎指向一致的概念与时序，都是前三个为负价值，最后一个为正价值，综合评价为需加工。显然这样的结果并不准确，而本节将解决此问题。

| 20101 | 分析问题：`制定解决方案` |
| --- | --- |
| 问题 | PM中，距与纬有雷同的经历，并且这种情况肯定也不太少数。 |
| 分析 | 从具象中找理性评价，似乎变的不太够用，也不太好用，且与反思的向性相违背。 |
| 向性 | PM是理性反思评价，而反思的向性应该是向上、向右。 |
| 方案1 | 是乌鸦吃的太少,再多吃一些,应该就可以了 `5％`; |
|  | 分析：违背向性原则，并且也不能解决根本问题，故应该不会采用。 |
| 方案2 | PM采用SP进行反思评价 `95%`; |
|  | 分析：此方案符合向性原则（小），且符合决策更好的使用TI知识的原则（中）。 |
| 方案3 | 在方案2的基础上，是否对SP再进行外类比，使其更确切 `50％`; |
|  | 分析：实现起来略麻烦，能不用尽量不用，实在不行，再考虑。 |

| 20102 | 方案2：PM采用SP进行反思评价：代码规划 |
| --- | --- |
| 1 | 使神经网络可视化，对SP节点支持不同色显示，以查看PM中是否抽象指向SP节点。 |
| 2 | 查下反向类比代码，使之仅支持`A有无`、`V大小`，不要有`V有无`。 |
| 3 | 查下反射类比代码，是MFo和PFo间类比，还是两个MFo类比的，看是否需调整。 |
| 结果12 | 经查，`PM并没有指向SP`，且`反向类比又很难执行到（靠不住）`。 |
| 结果3 | 经查为MFo和PFo间的类比，但这没什么问题，无需调整。 |
| 结论 | 方案2，看起来并不乐观，故重新制定新方案，见20103。 |

| 20103 | 新方案：解决PM理性反思评价的准确度问题 |
| --- | --- |
| 原则 | 我无需知道理性否，只需知道理性是。 |
| 例1 | 我无需知道脏的不能吃，只需知道干净的可以吃。 |
| 例2 | 我无需知道距27不能吃，只需知道距0、距3可以吃。 |
| 例3 | 我无需知道经278不能吃，只需知道经276能吃。 |
| 示图 | ![](assets/283_PM理性反思评价准确迭代示图.png) |
| 分析 | 直接根据P独特码，找是否指向`理性是`，是则能吃。如果未指向`理性是`，则加工。 |
| 方案4 | 从外类比中，生成`理性是`节点，比如距3能吃。`5%` |
|  | 分析：太麻烦，尽量不用，实在不行，再考虑。 |
| 方案5 | 直接在PM从conPorts中找到`理性是`所经历的经验。`95%` |
|  | 分析：现在PM就这么做的，但实测中并不行，明天结合实测再分析下。 |

| 20104 | 新方案: 用模糊与相对确切的角度,解决PM评价准确度问题 |
| --- | --- |
| 实训日志 | ![](assets/284_PM评价准确度问题之用模糊与相对确切角度解决日志图.png) |
| 示图Q | 图中,dis和y有类似的情况,但要求dis加工为0,而y即不必加工,如何兼顾二者? |
| 示图A | 需找出dis和y的区别,此二者区别为:dis已较为确切,而y确还模糊,但智能体不知其模糊状态 (即智能体对模糊或相对确切的经验代码处理方式是一致的); |
| 方案6 | 加大对y的训练量,使其认识到任何y都可以吃 `95%`; |
|  | 注意: 此处与`远果不能吃`的训练相矛盾,即不用刻意再训练`远果不能吃`; |
|  | 分析: 这会影响到`反向类比`的调用率吗?事实上,真正的mv-前置时序,比如[车,撞],其抽象时序[物体,撞],是真实发生的mv-时序,而躲避汽车并不会导致mv+,所以反向类比,不应以同区正负mv时序时才触发类比; |
|  | 分析2: 将预测[车,撞]->{mv-},改为预测非{mv-}时,即可进行反向类比,因为理性上已经成功避免mv- `转至n20p12` |

| 20105 | 针对方案6的训练 |
| --- | --- |
| 说明 | 飞到左下角,然后一飞一直投,一直飞到右上角为止,打出日志发现多为构建F4060节点; |
| 示图 | ![](assets/285_PM评价准确度问题方案6训练可视化.png) |
| 问题 | 发现F4060节点,只有两个具象,并且这两个具象没有再指向具象节点; |
| BUG | 右下角,边飞边直投到右上角,结果其抽象时序只指向两个具象,查下为什么; |
|  | 调试:发现新时序[坚果,吃]很难识别到结果,转n20p11;  |

| TODO | STATUS |
| --- | --- |
| 1. 因20104方案6,废弃综合评价,改为同区价值时,需要同向; | T |
| 2. PM评价,不同区价值时,其和需>0 `先不做`; |  |


<br/><br/><br/><br/><br/>

### n20p11 TIR_Fo识别率迭代
`CreateTime 2020.07.17`

　　在20105中，做左下边飞边吃到右上训练时，发现新的[坚果,吃]时序很难识别成功，本节将针对此问题，对TIR_Fo算法进行迭代。

| 20111 | 旧TIR_Fo算法模型图 |
| --- | --- |
| 示图 | ![](assets/286_旧TIR_Fo模型图.png) |
| 说明 | 由图可见,(吃)可能有上万条refPorts,前十条极可能都是非常具体的坚果,根本不可能全含A81抽象果; |
| 如图 | ![](assets/288_旧TIR_Fo模型识别率低的原因.png) |
| 说明 | 图中proto是F4545,assFo是F1265,要全含就需要A81是A311,但事实上相反A311是A81; |
|  | ![](assets/289_旧TIR_Fo的模型识别率低的原因可视化.png) |

| 20112 | 新TIR_FoV2算法模型图 |
| --- | --- |
| 示图 | ![](assets/287_新TIR_Fo模型图.png) |
| 说明 | 新模型中,用取交集计数的方式排序,是更理性的做法,自然能够提升识别率; |
| 进阶 | 可以添加一条要求,比如全含且必须包含最后一位(吃),以减少范围,提升性能; T |
| 测试 | 写完后,经训练测试,发现v2用`计数排序`的方式`不利于找出更确切的抽象结果`,故`废弃`; |

| 20113 | TIR_FoV1.5算法 |
| --- | --- |
| 示图 | ![](assets/286_旧TIR_Fo模型图.png) |
| 说明 | 与v1逻辑一致,只是整理了下代码,更为v1.5方法名; |

| TODO | STATUS |
| --- | --- |
| 1. 对时序识别v2算法,支持末位帧必包含; | T |

<br/><br/><br/><br/><br/>

### n20p12 减少依赖反向价值
`CreateTime 2020.07.17`

因为反向价值太少见,所以对其依赖才工作,将变得很难,将价值正与负的依赖改为:
1. 预测正变成预测不为正;
2. 预测负变为预测不为负;

| TODO | STATUS |
| --- | --- |
| 1. TI时,对输入短时记忆上下帧进行价值比较时,用正与非正,负与非负,替代反向类比; |  |
| 2. TO时,决策的任务,仅追求正价值,与避免负价值,而不必找SP兄弟节点解决; |  |

<br/><br/><br/><br/><br/>

### n20p13 用时序识别V2_继续三测
`CreateTime 2020.07.19`

| 20131 | 训练步骤 |
| --- | --- |
| 1 | 边投边飞,左下至右上; |
| 2 | 右投,飞过去吃掉; |
| 3 | 右投,饿,PM想飞,但找不到GL结果; |

| 20132 | 训练中,观察日志发现-神经网络问题: |
| --- | --- |
| 1 | 坚果未抽象,识别坚果概念老是相似而非全含; |
| 2 | 直投时序识别总失败; |
| 3 | 触发的外类比太少; |
| 分析 | 3导致1: 触发外类比太少,导致时序抽象不够,同时坚果也未抽象 (所以老相似而非全含); |
| 分析 | 3导致2: 外类比太少,导致时序抽象不够,所以导致直投时序识别总失败 (因为无法全含); |
| 结果 | 即3导致1和2,只要解决3的问题即可 `转20133`; |

| 20133 | 时序外类比触发太少问题 |
| --- | --- |
| 分析 | 外类比是由短时记忆各帧mModel.matchFo触发,且要求matchFo必须指向有效的价值; |
| 问题 | 输入TIR和TIP都会构建时序,其实TIR理性时序占大多数,所以matchFo.cmv_p常为空; |
| 解决 | mModel.matchFo源于识别,所以在识别中,应当以有价值影响为优先,即可解决此问题; |
|  | ![](assets/290_分析时序识别是否以有价值影响优先.png) |
| 说明 | 上图中,虽然很少匹配到有价值时序,但也能够慢慢得到外类比抽象时序; |
|  | ![](assets/291_时序识别先不以有价值优先示图.png) |
| 说明 | 如上图,先上飞直投,后右飞直投,就得到了`F95[A94(速高距向皮),A1(吃)]->{mv+}`; |
| 最终 | 调整训练方式,为先原地上吃,右吃,来得到确切的`抽象时序与概念`,再进行下步训练,如果这样的训练方式,能够顺利进行下去,就先不对`时序识别算法与外类比触发`,做调整; |

| 20134 | 测试终止,原因 |
| --- | --- |
| 测试 | 在测试中发现,TIR_FoV2算法,用计数排序的方式,导致其不利于找出更确切的抽象时序; |
| 问题 | 问题在于,其识别出的时序总是具象的,未指向价值的; |
| 导致 | 这导致未指向价值的识别,是无法触发外类比的,从而无法抽象出更确切的时序; |
| 问题 | 最终,无法识别更抽象时序,也无法构建更抽象时序,形成恶性循环; |
| 解决 | 我们将v1算法整理成v1.5算法,替换上后,进行训练试试 `转n20p14`; |

| TODO | STATUS |
| --- | --- |
| 1. 对时序识别结果,优先识别为有价值影响的时序结果 `先不改,用调整训练慢慢得到`; | T |


<br/><br/><br/><br/><br/>

### n20p14 用时序识别V1.5_继续三测
`CreateTime 2020.07.21`

　　因V2计数排序导致不利于识别更抽象时序问题（参考20134），本节将切换时序识别算法至v1.5下，进行训练，并记录情况，分析问题。

| 20141 | 训练步骤 |
| --- | --- |
| 1 | `直投,右上飞,直投`: 得到`F14[A13(速高距向皮),吃]->{mv+}`; |
| 2 | `边吃边飞至左下`: 大多数识别并抽象为F14 |
| 3 | `边吃边飞至右上`: 大多数识别并抽象为F14 |
|  | 也用部分抽象为`F54[A52(高距向皮),吃]->{mv+}` (因为飞导致速度>0时导致); |
|  | 更小部分抽象为别的带飞的时序,较杂不记录; |
| 4 | `远投右`: 概念相似匹配为A61(速高距向皮,x143,y581),时序识别为F14 |
| 5 | `持续右飞`: 远距未触发外类比 `转至20142`; |
| 6 | `飞到坚果上时,吃掉`: 0距识别为F14 |

| 20142 | 构建无距果问题 |
| --- | --- |
| 问题 | 从1-6训练中,发现远果未触发外类比的问题: 即如何发现距0和远距都是(无距果)? |
| 解决 | 可在远果概念相似识别后,对其进行概念类比,并构建抽象; |
| 返回 | 将seemAlg作为识别结果返回,虽然构建了抽象无距果:`A190(速高向皮)`; |
| Q1 | 为什么不将构建的abs无距果返回,而是seemAlg? |
| A1 | 因为返回无距果,会导致其未被fo引用过,从而导致无预测,无外类比; |
| 总结 | 构建无距果,并未触发外类比,因为远投,无需外类比 (缺乏同向价值触发); |

| 20143 | 修改为`20142的构建无距果后,将seemAlg返回`后,再训练 |
| --- | --- |
| 1 | `重启,直投,右上飞,直投` |
| 2 | `重启,边吃边飞至右上` |
| 3 | `重启,边吃边飞至左下` |
| 4 | `重启,远投右`: `相似识别为:A61(速0,高5,距0,向→,皮0,经249,纬145) >> 构建抽象:A185(速0,高5,向→,皮0)` |
| 5 | `持续右飞` |
| 6 | `飞到坚果上时,吃掉` |
| 7 | `重启,右投,马上饿`: 发现循环行为输出:`右飞,吃`,停不下来,查下为什么? |

| 20144 | `右飞,吃`行为循环问题,日志分析 |
| --- | --- |
| BUG1 | ![](assets/292_20143训练BUG1_找到的解决方案不够抽象.png) |
|  | 1. 发现方向索引强度一直是64不变的bug,在抽象时,将其强度+1,后解决; |
|  | 2. 将方向索引后,理性取交集废除掉,参考`topPerceptModeV2迭代记录`; |
| BUG2 | ![](assets/293_20143训练BUG2_时序识别参数错误.png) |
| BUG3 | ![](assets/294_20143训练BUG3_训练过飞但找不到GL距离节点.png) |
| BUG4 | ![](assets/295_20143训练BUG4_Hav转移失败.png) |
|  | 1. 发现在_Hav中,relativeFos仅找理性交集的方案,但很难一蹴而就,参考_Hav算法200727迭代记录 |
| BUG5 | ![](assets/296_20143训练BUG5_思维活跃度每轮循环时重置导致消耗不尽.png) |
|  | 1. 仅在DemandManger中有新需求时,才加思维活跃度,避免决策不停循环; |
| 说明 | 以上几个BUG,不断重复着,逐一解决即可; |


<br/><br/><br/><br/><br/>

### n20p15 在20143上_继续三测
`CreateTime 2020.07.27`

上节中,对20143的训练进行了测试,并对五个bug进行了修复,本节在修复后回归测试,再测修bug;

| 20151 | 继续测试20143 |
| --- | --- |
| 说明 | 训练至第7步,马上饿后; |
| BUG1 | **发现一轮轮循环中,TOP.P+不应期总是0条,未生效?** |
|  | ![](assets/298_循环中不应期总为0的BUG.png) |
| BUG2 | **解决方案[A8(0距果),吃]的A8.cHav,其relativeFo为[飞↗,A8(有)],每轮循环都在重复飞↗行为,而此时其实relativeFos有两个元素,另外一个也许有其它前提,比如要求有`左下角果`;** |
|  | ![](assets/299_循环中每轮都对A8的同Fo行为化BUG.png) |
| BUG3 | **对时序预测中,未发生的事,也参与了内类比?** |
|  | ![](assets/300_对matchFo未发生的部分也内类比的BUG.png) |
| BUG4 | **PA在识别期抽象为MA,在_Hav中优先MC时,是不是就应该直接转到PM加工?** |
|  | ![](assets/301_在_Hav中MC优先未正常执行的BUG.png) |
|  | 此处,未能及时MC理性评价,`引出n20p16` |
|  | 查可视化:![](assets/303_20151BUG4可视化.png) |
|  | 说明:上图中,M和C可以加工,但问题在于如何判断其需要加工? |
|  | 分析:其实就是M与C如何创建共同抽象,或者cIsM(距0也是无距果)关联; |
|  | 分析2:关于M和C的抽象,只有从外类比和内类比两种方式; |
|  | 分析3:0距和非0距很难进到外类比,所以只能从内类比来触发试试了; |
|  | 分析4:先扔个飞一两步距离的远果,然后飞成0,看内类比,是否有对达到0果做内中外类比,从而获取M和C的抽象关联; |
|  | 结果1: 训练可类比构建出抽象无距果,见`20153-第5步` |
|  | 结果2: 做了TODO5,6改动后,回归训练后,顺利执行MC `参考20153-6`; |
| BUG5 | _GL行为化中,找glAlg时,距小相对节点的具象都为空,导致联想glAlg失败; |
|  | ![](assets/307_20151BUG5联想glAlg具象指向为空.png) |
|  | 解决: 将protoFo和matchAFo的构建改为isMem=false,从而内类比构建时序时,具象指向为持久化时序,便可解决此bug `参考dataIn_NoMV()`; |
| BUG6 | BUG5解决后训练,发现新BUG; |
|  | ![](assets/308_20151BUG6_getInnerAlg方法返回不相干结果.png) |
|  | 分析: 经调试,在第一步,取innerValue_p时就以小却取了大; |
|  | 怀疑: 怀疑是内中外类比,导致抽象出飞GL与距GL形成时序,具体BUG源头在构建期; |
| BUG7 | 在原训练基础上继续训练,就是`再重启,右投,马上饿`,发现以下BUG`不复现` |
|  | ![](assets/309_20151BUG7_距小经验太少的问题.png) |
|  | 结果: 重新训练,未复现,`参考:BUG8中重新训练结果` |
| BUG8 | 并且为什么,距离变小的时序,是F21[A8(飞大)]?,这显然是不对的;`不复现` |
|  | 分析: 经查,A20.refPorts只有F21,但F21的content_ps却没有A20,二者关联错乱; |
|  | 结果: 重新训练,希望找出这种错误关联是如何构建的,但未复现,见下图; |
|  | ![](assets/310_20151BUG8_不复现.png) |
| BUG9 | 方向索引强度异常; `不复现` |
|  | ![](assets/311_20151BUG9_方向索引强度异常.png) |
|  | 分析: 重新训练,将强度>100时打断点; |
|  | 查明: 经查,地址会重复,估计是因为`[[NSUserDefaults standardUserDefaults] synchronize];`无法及时将数据写入导致; |
| BUG10 | PM中任务分为负的BUG `T` |
|  | ![](assets/312_20151BUG10_PM任务分为负.png) |
|  | demand本来就为负,解决方案才为正,所以改为取-demand.score即可; |
| BUG11 | MC中发现"无速果"的问题 (因为坚果不移动,不应该有无速果); `T` |
|  | 经查,有两种可能导致 |
|  | 1. 还没吃完上个,就飞下个 `5%` (加快动画即可解决); |
|  | 2. ios的view复用 `95%` (记录view.init时间,以判断唯一性,可解决); |
| BUG12 | 决策完成,但没有解决demand时,却停止了; |
|  | ![](assets/313_20151BUG12_决策完成未解决demand的停机问题.png) |

| 20152 | 决策递归嵌套 |
| --- | --- |
| 说明 | 综20151,不应期的递归,应先对subModel进行不应期递归,后逐级递归至parentModel; |
| 示图 | ![](assets/297_决策递归嵌套.png) |
| 说明 | 图中,上下级的递归,嵌套着每一个subModel的子递归; |
| 中止 | 本来就是递归嵌套的,但上述BUG主要是因为MC未执行,与递归嵌套无关。 |

| 20153 | 修改BUG4后,回归训练 |
| --- | --- |
| 1 | `重启,直投,右上飞,直投` |
| 2 | `重启,边吃边飞至右上` |
| 3 | `重启,边吃边飞至左下` |
| 4 | `重启,远投右,持续右飞,飞到坚果上时,吃掉` |
| 5 | `重启,远投右,持续右飞,飞到坚果上时,吃掉` (此处抽象出无距果,如下图) |
|  | ![](assets/305_内类比构建抽象无距果示图.png) |
| 6 | `重启,右投,马上饿` |
|  | ![](assets/306_20151BUG4修复并顺利执行MC.png) |

| 20154 | 简化训练步骤 |
| --- | --- |
| 1 | `直投,右下飞,直投,边吃边飞至右上` |
| 2 | `重启,右投,飞至坚果,吃掉` |
| 3 | `重启,右投,马上饿` (小鸟自行飞到坚果并吃掉); |

| TODO | STATUS |
| --- | --- |
| 1. BUG4_内类比,使用protoAlg进行; | 本来就是 |
| 2. BUG4_fo[距47果,飞右,距0果]下,MC时,当时序为M的具象时,即可加工; |  |
| 3. BUG4_fo[距47果,飞右,距0果]下,MC时,时序中的alg为M的具象时,用fuzzy排序匹配,再对MC的稀疏码进行加工; | 太复杂,不采用 |
| 4. BUG4_修复subModel的不应期,看下第二方案中是什么内容: [(A8有)]; | 无用内容 |
| 5. BUG4_内类比大小,使用protoFo进行,有无使用matchAFo进行; | T |
| 6. BUG4_在内类比有无时,将内类比的两个概念类比抽象; | T |

<br/><br/><br/><br/><br/>

### n20p16 评价器
`CreateTime 2020.07.28`

　　过去的很长时间，我们在决策端的迭代总是更多，而最终都将责任大多落到了类比期，而决策期真的将知识应用到了极致吗？显然不是，在上节20151_BUG4中，发现MC理性评价器并未及时工作，导致小鸟在训练中缺乏理性的一味的向右上飞行（当然不应期未正常工作也是BUG原因之一），本节将对评价器进行整理，从而从决策模型上根本解决此问题，使决策能够更充分的使用神经网络知识，使TO代码更直接的被决策模型所指导。

| 20161 | 评价器 |
| --- | --- |
| 说明 | 类似`类比器`,我们尝试封装`评价器`,模型如下: |
| 模型 | ![](assets/302_评价器模型.png) |
| 说明 | 1. 理性评价器REvalute和感性评价器PEvalute,共两个; |
|  | 2. PE类似外类比,对mModel.MFo与解决方案fo间,进行加工,使其为价值正; |
|  | 3. RE类似内类比,与mModel.M/PAlg与解决方案的Alg间,进行加工,使其稀疏码趋近; |
|  | 4. 类比器是从`输入-类比-构建抽象`,而评价器是从`应用具象-评价-输出`; |
| 总结 | 现在的代码都有支持,TOP的模式是对PE的支持,而MC/PM是对RE的支持; |
|  | 现在先不封装评价器,v3.0时再说,目前的评价出现问题可先由此模型先指导; |


<br/><br/><br/><br/><br/>

### n20p17 训练飞行方向
`CreateTime 2020.07.31`

| 20171 | 内类比-理性知识支撑 |
| --- | --- |
| 说明 | 不同方向的坚果,需要向不同方向飞,这种知识需要通过内中外类比获得; |
| 示图 | ![](assets/304_训练飞行方向之内类比迭代.png) |

| 20172 | BUG解决 |
| --- | --- |
| BUG1 | 默认方向右导致方向无法被抽象剔除; |
|  | ![](assets/314_20172BUG1_默认方向右导致方向无法被抽象剔除.png) |
|  | 方案1: 视觉算法改为: 距0时,无方向 `简单,优先采用`; |
|  | 方案2: 直投时,有点位置偏差,能吃到,但每次投的方向也有所不同,终抽象出无向果; |
|  | 分析: 先用方案1试下,不行再用方案2; |
|  | 方案2修复后回测: ![](assets/315_20172BUG1方案2修复后回测.png) |
|  | 方案3: 再多训练几轮边飞边吃,看能否将P识别为(无距,无向果); |

| 20173 | PM评价BUG |
| --- | --- |
| 说明 | 1. 当"远投,饿"吃到的,和"远投,饿"未吃到的,都发生后; |
|  | 2. 在PM评价中,因为未吃到过,就评价为否,导致PM修正失败; |
| 分析 | 1. 比如(向左下,距23)->{mv-},显然导致失败原因是距离,而不是方向; |
|  | 2. 但PM中,是逐个稀疏码判定的,导致此处`方向`评价为否,并修正失败; |
|  | 3. 看起来,是PM的评价不够充分,或者太简单,导致无法支撑本节多方向飞行训练; |
| 方案 | 1. 能不能提前,分析出距离导致的问题 (即更综合的PM评价); |
|  | 2. 类似经纬当时的处理方式 (即多经历,来使模糊匹配到不必修正); |
|  | 3. 对`距离导致`进行抽象 (需抽象`距>0`,或者支持from-to节点); |
| 选方案 | 方案1,综合永远都有错误率,并不能根治问题,还增加了不确定性 `0%`; |
|  | 方案2,已在经纬时实测可解决,但有较小错误率,如正好fuzzy碰到了负价值; |
|  | 方案3,此方案解决此问题最彻底且理性,但改动较大,可重点分析这条 `95%`; |
| 结果 | 1. 可尝试从理论模型上出发,分析方案3 `90%`; |
|  | 2. 可考虑方案2,先解决了问题再说 `10%`; |
| 方案3 | 采用反向类比SP知识,先调试下看,fuzzy找到的概念,是否有brother节点; |
| 示图 | ![](assets/316_PM评价BUG方案3示图.png) |
| 说明 | 图中,表明先`反向类比`构建SP,再进行`PM`使用SP经验的方式; |
| 问题 | 问题在于,A2不会指向mv+,如果指向了,那么A2就是0距果,而P是不可能被识别为0距果的,所以反向这个条件是不成立的,除非是mv负与mv平,进行反向类比,那么反向类比就有点像`感性内类比`,即什么数据导致了价值负 `转至n20p18`; |
|  | 方案3结果: 本来就不是坚果导致饿的,所以无论是`向↙`还是`距23`,都不是饿的原因,转至以下方案4; |
| 方案4 | 先点马上饿,再远投; |
|  | 分析: 因为本来就非坚果导致饿,并且早期的偶然事件对HE的知识体系影响较大,所以可以从先饿,再扔坚果,来尝试训练多向飞行; |

| 20174 | 尝试用方案4训练 |
| --- | --- |
| 示图 | ![](assets/317_PM评价BUG方案4训练.png) |
| 问题 | 图中,我们仅知道`距0果`可以吃,但并不知道`距26果`能不能吃; |
| 分析 | 如果仅基于fuzzy匹配,永远都无法获得更确切的解,因为: |
|  | 1. fuzzy本来就是多变的,不确定因素较大; |
|  | 2. 只有抽象才是解决`确切性`问题的,所以还是得思考下从SP出发解决; |
| 分析 | 原先`指向价值负`时PM评价失败,显然是不准确的,因为有可能负价值从未发生,(`正价值`相反是`非正价值`,而不是`负价值`) `转至n20p18`; |

<br/><br/><br/><br/><br/>

### n20p18 PM评价BUG-反向反馈类比迭代为反省类比
`CreateTime 2020.08.12`

　　在20173方案3的316示图中，发现了最可能的是负与平进行类比，而非负与正。那么这种类比方式就更像一种`感性外类比`，或者叫`非正向反馈类比`。如果把外类比当做取并集，那么`非正向类比`就是取差集。以往我们已经写好了反向反馈类比算法，但因为这个原因，其实很难触发，基于此，我们有理由对其进行迭代，将`反向类比`迭代为`非正向类比`。

> 命名: 因反向反馈类比,不再是反向,而是`正与非正`,而针对的时序,也由仅支持`预测时序`改为,支持决策短时记忆中所有时序,所以本节,将其更名为:`反省类比`;

| 20181 | 20174-PM评价BUG-理性感性两种解决方向 |
| --- | --- |
| 1. 理性 | 理性远果吃不掉 (无法变成cNone果); |
|  | 难点: 如何触发`果无`失败,并理性发现远果吃不掉; |
| 2. 感性 | 感性远果不能吃 (mv非正); |
|  | 难点: 如何触发`非正`的反馈类比; |

| 20182 | PM评价方案-参考旧方案 |
| --- | --- |
| 旧评价示图 | ![](assets/318_PM评价旧方案示图.png) |
| 说明 | 常在河边走,总有湿鞋时,这种依赖fuzzy撞大运,本身就违背熵减的原则; |

| 20183 | PM评价方案-制定新感性方案 |
| --- | --- |
| 分析 | 1. 抽象出cPlus,如距0可mv+; |
|  | 2. 抽象出cSub,如距23不可mv+; |
| 难点 | 只要可以触发反向反馈类比,即可从中分析S和P,问题在于触发方案; |
| 触发方案 | 时序元素间追加生物钟时间间隔,并制定触发策略,如>130%时,触发; |
| 触发原理 | 当生物钟超过触发策略阈值时,触发 |
| 反省类比 | 1. 理性_价值`正与非正`间,进行反向反馈,并反省原因 `参考n18p6`; |
|  | 2. 感性_可考虑产生焦急情绪,再反向反馈,并反省原因 `参考n18p7`; |
| 结果 | 暂不支持`焦急`触发,因为只有`生物钟`是必做的,`转至20184`; |

| 20184 | 激进方案: 迭代反省类比 |
| --- | --- |
| 简介 | 接20183,先做`生物钟反省触发`和`决策时序全面支持`功能; |
| 示图 | ![](assets/319_预想与实际进行反省类比.png) |
| 说明 | `参考:18061_反省方式`,图中对每一个foOutModel未正常执行时,都进行反省反思,并得到非常丰富的SP抽象; |
|  | 在PM评价时,可以因此而快速得到非常理性的SP知识,并以此得到评价结果; |
| 结果 | 转至n20p19,对此方案进行代码规划; |

| 20185 | 保守方案: 暂不迭代反省 |
| --- | --- |
| 简介 | 以当下的问题出发,改进训练方式,尽量不迭代反省,在原反向类比基础上解决; |
| 步骤 | 1. 左投飞过去吃掉。 |
|  | 2. 左投，预测到吃掉，马上饿，触发反向类比，发现是距离导致，而不是方向。 |
|  | 3. PM改用SP做评价基准。 |
| 难点 | 细致分析知识网络变化,看这种方式能否支撑解决当下的问题; |
| 缺点 | 1. 这么做,其实是用指定训练方式代替了触发; |
|  | 2. 并且这种触发一般都是偏感性的,不如`生物钟`方式理性; |
|  | 3. 且这种触发非常不全面,比如针对各层subFoOutModel,显然无法照顾到; |
|  | 4. 这种触发方式本身不符合`熵减机将原始熵信息进行输入`的原则,因为要求训练方式,其实就是要求了熵减,而再遇到熵增时,则无法自行处理; |
| 结果 | 基于以上缺点分析,不必再考虑这条保守方案 `废弃`; |


<br/><br/><br/><br/><br/>

### n20p19 反省类比代码规划
`CreateTime 2020.08.14`

> 反省类比的作用:
> 1. `反省类比`为错误的行为尝试开放了豁口,即允许在未知之下试错输出,一旦学会,就能快速习得经验,并帮助其不会再犯;
> 2. `反省类比`的触发,不要求input输入,即即使不输入,也会触发,比如等饭,结果三小时了还没好;  
> ![](assets/319_预想与实际进行反省类比.png)

| 20191 | 激进方案: 迭代反省类比代码规划之:生物钟 |
| --- | --- |
| 类型 | 1. 客观时间,即智能体年龄 (即从出生到现时刻时长,单位ms) |
|  | 2. 主观总体,即智能体运行时时间 (即运行时计数,关机停计,单位ms) |
|  | 3. 主观细分,在2的基础上精简,仅作用于当下时序构建; `选定` |
| 使用 | 1. 每个概念构建到瞬时记忆时,将当前时间戳计入AIShortMatchModel中; |
|  | 2. mv输入时,计时间戳到(尽量只放字典,其次或放mvNode); |
|  | 3. 瞬时记忆构建时序时,以首位为0,向右分别计deltaT,末位计mvDeltaT; |
|  | 4. 抽象时序时,首位归为0,此后每位间,deltaT从具象中取较大的值; |

| 20192 | 激活方案: 迭代反省类比代码规划之:触发器 |
| --- | --- |
| 代码 | 写触发器,制定几下几点功能; |
|  | 1. timer计时器触发,取deltaT x 1.3时间; |
|  | 2. 将预想时序fo,和实际时序fo存至触发器中; |
|  | 3. 当outModel中某时序完成时,则追回(销毁)与其对应的触发器; |
|  | 4. 直到触发时,还未销毁,则说明实际时序并未完成,此时调用反省类比; |
| 原则 | 将commitFromOuterPushMiddleLoop()中mIsC匹配上的,都存Tigger中做为实际发生,以使反省类比全面到位; |
| 定义 | SP也是模糊到相对确切的,起初无SP时,PM算法不执行修正,直到通过反省类比确切后,才会有修正; |
| 实例 | 理性例: 小孩子会尝试取到视频通话手机屏中的水果,但几次拿不到,反省发现(水果在屏幕中),从此不再通过屏幕拿东西; |
|  | 感性例: 小孩子抢吃的会开心,有次抢了大人白酒,喝了辣不开心,下次不抢了; |

| 20193 | 激活方案: 迭代反省类比代码规划之:类比器 |
| --- | --- |
| 代码 | 1. 对反向反馈类比进行迭代,使其更好的支持`反省类比`的一些特性; |
|  | 2. 形成理性而丰富的SP抽象; |

| 20194 | 激活方案: 迭代反省类比代码规划之:反省结果应用 |
| --- | --- |
| 1 | 在PM评价中,对这些反省类比构建的SP抽象进行应用; |

| TODO | STATUS |
| --- | --- |
| 1. 在AIShortMatchModel中写inputTime; | T |

<br/><br/><br/><br/><br/>


### n20p20 反省类比代码实践
`CreateTime 2020.08.18`

| 20201 | 时序的deltaTimes规则 |
| --- | --- |
| 示图 | ![](assets/320_时序deltaTimes规则.png) |
| 说明 | 图中,蓝色数字为deltaTimes间隔; |
|  | 1. 在具象时序时,将inputTime直接传入; |
|  | 2. 如图:在抽象时序时,取Max(各间隔间); |

| 20202 | 触发器代码规则 |
| --- | --- |
| 示图 | ![](assets/321_反省的两种触发器.png) |
| 1 | 在输出行为(ActYes)时,构建理性触发器(fo.deltaTimes); |
| 2 | 决策整体成功(Demand.status=Finish)时,构建感性触发器(mvDeltaTime); |
| 3 | 当_Hav中ATType为HNGL时,构建理性触发器(fo.deltaTimes),目标为HNGL的变化; |
| 4 | 当触发器超时(deltaTx1.3)后,触发; |
| 5 | 触发后,先判断此时的任务是否还在等待状态(ActYes); |
| 扩展 | TIR_Fo预测时,是否也构建触发器,`比如汽车红灯停,警车不停;` |
| TODO | 统一由预测构建触发器,而不是`ActYes` `思考下是否真要这么做`; |

| 20203 | 触发器代码步骤 |
| --- | --- |
| 1 | 三处构建触发器: (最好到操作的TOModel中构建触发器) |
|  | > a. demand.ActYes处 |
|  | > b. 行为化Hav().HNGL.ActYes处 |
|  | > c. 行为输出ActYes处 |
| 2 | 外循环回来,把各自实际输入的概念,存入到TOAlgModel/TOFoModel中 (最好在PM中存,因为PM在对其进行处理,面向的数据全); |
|  | > a. 改为新写commitFromOuterInputReason方法,进行处理; |
|  | > b. 相符判断: "isOut输出"和"demand完成"和"HNGL.H"时,直接根据mIsC判断外循环输入是否符合即可; |
|  | > c. 相符判断: 其中GL的相符判断,转20204; |
| 3 | 当生物钟触发器触发时,如果未输入有效"理性推进" 或 "感性抵消",则对这些期望与实际的差距进行反省类比; |

| 20204 | GL的真实输入相符判断与保留 |
| --- | --- |
| 简介 | GL的相符判断,因为涉及到稀疏码变化,与概念输入的判断不太一样,如下图: |
| 示图 | ![](assets/322_外循环保留真实概念之GL处理示图.png) |
| 示例 | GL处理时,需要对`期望`与`真实`概念`除变化特征外`是否有共同抽象做判断,比如我希望坚果变近,但近处出现一碗面; |
|  | 1. 出现面很正常,因为我以前经历过多次,一飞近坚果就出现面 / 坚果变面; |
|  | 2. 出现面不正常,本来坚果和面都存在,我飞近坚果,结果只有面变近了; |
|  | 分析: 在确切化之前,二者都可能,而确切化后,经验就要帮助到此处运作; |
|  | 结论: 暂不深究,直接判断`期望`与`真实`概念的absPorts是否有交集; |

| 20205 | 反省类比 |
| --- | --- |
| 示图 | ![](assets/387_反省类比示图.png) |
| 原则1 | 无需收集realFo,直接对每个subAlgModel单独取用处理即可; |
|  | 直接将status=ActYes的TOModelBase交由反省类比算法,算法对每个subAlgModel中取出realContent_p,并单独进行类比,取得结果构建成ATSub抽象; |
| 原则2 | 其实反省类比,并没有进行类比,而是将决策中的即有的类比结果收集起来; |
|  | 对TOAlgModel中未修正(或无需修正)的独特稀疏码,进行收集,作为类比结果; |
| 原则3 | 反省类比的结果再通过外类比确切化; |
|  | 上次阿三和李四拖后腿,这次是阿三和王五,那么认为阿三才是主菜; |
| 步骤 | 触发后的代码步骤如下: |
|  | 1. 将每一帧中未被PM修正的稀疏码,构建成ATSubAlg; |
|  | 2. 将所有帧ATSubAlg再连起来,构建成ATSubFo; |
|  | 3. 根据subFo,从稀疏码向概念,再向时序索引查找,同样foNode的另外的assSubFo,并进行外类比; |
|  | 4. 外类比构建更确切的S时序,如果已存在,则加强; |

| 20206 | 将反省结果应用于PM理性评价 |
| --- | --- |
| 结构图 | ![](assets/326_SP用于PM理性评价短时记忆示图.png) |
| 说明 | 如图,PM操作M模型的base为当前帧,再base为当前方案时序; |
| 步骤 | 1. 取outModel.base,得到当前帧curAlg; |
|  | 2. 取outModel.base.base,得到当前解决方案curFo; |
|  | 3. 根据curFo取ATSubPorts,查对应在curAlg上是否长过教训; |
|  | 4. 或者curFo中,没有距>0的概念,所以查下是否对预期Fo将具象概念替换进去,然后再形成时序和ATSub抽象指向 (废弃此步,因为我也看不懂当时写的啥意思来着); |
| 步骤图 | ![](assets/327_SP用于PM理性评价示图.png) |
| 说明 | 如图,以Fo.absPorts中取ATSub部分,作为SubAlg的有效判断; |
|  | 并对有效的SubAlg中同区稀疏码,进行值排序,与P特有码最接近的为准; |
| 向性 | **理性评价的向性是从右至左,从时序->概念->特征->稀疏码;** |
| 原则 | **因向性原则,理性判断,要由右至左,即优先时序判断,而非特征稀疏码;** |
|  | 比如: 之所以张三的苹果我不能吃,是因为吃了会气死张三,而不是因为苹果离张三更近; |

| 20207 | PM理性评价之取GL值 |
| --- | --- |
| 示图 | ![](assets/328_PM理性评价之取GL值.png) |

| TODO | STATUS |
| --- | --- |
| 1. 构建具象时序,将inputTime输入到deltaTimes中; | T |
| 2. 构建抽象时序,自动从具象中提取deltaTimes; | T |
| 3. 在[relateFo:mv:]时,将mvNode的inputTime输入到mvDeltaTimes; | T |
| 4. 写singleLoopBackWithActYes()流程控制方法; | T |
| 5. 在OuterPushMiddleLoop支持inputMv抵消demand,其下fo设为Finish; `改由原DemandManager实现` | T |
| 6. 在TOFoModel中集成支持timeTrigger();`改到ActYes流程控制中` | T |
| 7. 在OuterPushMiddleLoop,waitModel为ActYes且为HNGL时,仅判定其是否符合HNGL变化,并设定为OuterBack状态; | T |
| 8. demand.subFo结束,调用在ActYes流程控制中构建触发器; | T |
| 9. 行为化_Hav的HNGL,调用在ActYes流程控制中构建触发器; | T |
| 10. 行为化行为输出时,调用在ActYes流程控制中构建触发器; | 暂不写 |


<br/><br/><br/><br/><br/>


### TODOLIST

| TODO | DESC | STATUS |
| --- | --- | --- |
| 20200716 | 参考20104末行说明+分析2,对反向类比进行迭代; | 转n20p12 |
| 20200716 | 参考20104末行说明,对R-算法进行迭代,使其不依赖SP兄弟节点解决; | 转n20p12 |


<br/><br/><br/><br/><br/>
